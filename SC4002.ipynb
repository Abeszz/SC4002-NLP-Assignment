{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Abeszz/SC4002-NLP-Assignment/blob/main/SC4002.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Installation & Requirements :"
      ],
      "metadata": {
        "id": "fRPGfwCv_acw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pip install datasets"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "J81YDyhL_mX5",
        "outputId": "9fc684bb-e410-484f-b372-bcef54d9ba95"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: datasets in /usr/local/lib/python3.10/dist-packages (3.0.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from datasets) (3.16.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from datasets) (1.26.4)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (16.1.0)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.3.8)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets) (2.2.2)\n",
            "Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.10/dist-packages (from datasets) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.10/dist-packages (from datasets) (4.66.5)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from datasets) (3.5.0)\n",
            "Requirement already satisfied: multiprocess in /usr/local/lib/python3.10/dist-packages (from datasets) (0.70.16)\n",
            "Requirement already satisfied: fsspec<=2024.6.1,>=2023.1.0 in /usr/local/lib/python3.10/dist-packages (from fsspec[http]<=2024.6.1,>=2023.1.0->datasets) (2024.6.1)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets) (3.10.10)\n",
            "Requirement already satisfied: huggingface-hub>=0.22.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.24.7)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from datasets) (24.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (6.0.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (2.4.3)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (24.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (6.1.0)\n",
            "Requirement already satisfied: yarl<2.0,>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.15.2)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (4.0.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.22.0->datasets) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (2024.8.30)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2024.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from yarl<2.0,>=1.12.0->aiohttp->datasets) (0.2.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install nltk"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "ZZ7IipO2Bmo1",
        "outputId": "ad2cb880-8c6f-455d-9418-45f76a69e52e"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (3.8.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk) (1.4.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk) (2024.9.11)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk) (4.66.5)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install numpy"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "wzYNgiwEBtoL",
        "outputId": "c92815ec-80c8-4c93-9a1d-c84e2810aa97"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (1.26.4)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install npm"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "3pFMpM5qpLFn",
        "outputId": "638d9c8d-5d98-420b-8a33-945d350d1d4a"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: npm in /usr/local/lib/python3.10/dist-packages (0.1.1)\n",
            "Requirement already satisfied: optional-django==0.1.0 in /usr/local/lib/python3.10/dist-packages (from npm) (0.1.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install torch"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0nqNXGOPywf8",
        "outputId": "c13c2fea-9ddc-4591-8271-ede14fc71776",
        "collapsed": true
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.4.1+cu121)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch) (3.16.1)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch) (4.12.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch) (1.13.3)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.4.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch) (2024.6.1)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (3.0.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch) (1.3.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "glove_file_id = '17CUd7jxuh6ptIljKaz_9gJQ8-40JXJ-F'\n",
        "glove_file = 'glove.6B.100d.txt'\n",
        "!gdown {glove_file_id} -O {glove_file}"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IFIJIoS9DuVa",
        "outputId": "4f1b709f-e539-43ef-8641-d723e2cfcfd8",
        "collapsed": true
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading...\n",
            "From (original): https://drive.google.com/uc?id=17CUd7jxuh6ptIljKaz_9gJQ8-40JXJ-F\n",
            "From (redirected): https://drive.google.com/uc?id=17CUd7jxuh6ptIljKaz_9gJQ8-40JXJ-F&confirm=t&uuid=046e18c6-43ca-429d-831f-2048deec21dc\n",
            "To: /content/glove.6B.100d.txt\n",
            "100% 347M/347M [00:07<00:00, 45.2MB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import nltk\n",
        "import sys\n",
        "\n",
        "env_base_path = sys.prefix\n",
        "nltk_path = os.path.join(env_base_path, 'nltk_data')\n",
        "nltk.download('punkt', nltk_path)\n",
        "nltk.download('punkt_tab', nltk_path)"
      ],
      "metadata": {
        "collapsed": true,
        "id": "B8qRXCT2GpKd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4aaf10a4-8bcb-404e-f395-c1a84191c940"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /usr/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package punkt_tab to /usr/nltk_data...\n",
            "[nltk_data]   Package punkt_tab is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset\n",
        "dataset = load_dataset('rotten_tomatoes')"
      ],
      "metadata": {
        "id": "iBA7sNlf9hW5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "99cdcdfb-e1a5-48b7-e402-112617a27eb2"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_token.py:89: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset = dataset['train']\n",
        "validation_dataset = dataset['validation']\n",
        "test_dataset = dataset['test']"
      ],
      "metadata": {
        "id": "o_TvlwmgB35t"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "import numpy as np\n",
        "from nltk.tokenize import word_tokenize\n",
        "from collections import Counter\n",
        "import csv"
      ],
      "metadata": {
        "id": "cBY500zfFszo"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Global variables\n",
        "UNKNOWN_TOKEN = '<UNKNOWN>'"
      ],
      "metadata": {
        "id": "rDD4AHuqWu_h"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Functions to build vocabulary and create embedding matrix\n",
        "def build_vocabulary(dataset, oov_handling_method='unknown_token'):\n",
        "    vocab_counter = Counter()\n",
        "    for sample in dataset:\n",
        "        tokens = word_tokenize(sample['text'].lower())\n",
        "        vocab_counter.update(tokens)\n",
        "    vocabulary = list(vocab_counter.keys())\n",
        "    if oov_handling_method == 'unknown_token':\n",
        "        if UNKNOWN_TOKEN not in vocabulary:\n",
        "            vocabulary.append(UNKNOWN_TOKEN)\n",
        "    return vocabulary\n",
        "\n",
        "def create_embedding_matrix(embedding_dim, vocabulary, glove_embeddings, oov_handling_method='unknown_token'):\n",
        "    vocab_size = len(vocabulary)\n",
        "    embedding_matrix = np.zeros((vocab_size, embedding_dim))\n",
        "    word_to_index = {word: idx for idx, word in enumerate(vocabulary)}\n",
        "\n",
        "    # Initialize special embeddings\n",
        "    if oov_handling_method == 'unknown_token':\n",
        "        # Use a single <UNKNOWN> token for all OOV words\n",
        "        unknown_vector = np.random.normal(scale=0.6, size=(embedding_dim,))\n",
        "        unknown_index = word_to_index[UNKNOWN_TOKEN]\n",
        "        embedding_matrix[unknown_index] = unknown_vector\n",
        "\n",
        "    # Fill the embedding matrix\n",
        "    for word, idx in word_to_index.items():\n",
        "        if word in glove_embeddings:\n",
        "            embedding_matrix[idx] = glove_embeddings[word]\n",
        "        else:\n",
        "            if oov_handling_method == 'unknown_token':\n",
        "                embedding_matrix[idx] = embedding_matrix[unknown_index]\n",
        "            elif oov_handling_method == 'random':\n",
        "                embedding_matrix[idx] = np.random.normal(scale=0.6, size=(embedding_dim,))\n",
        "            elif oov_handling_method == 'none':\n",
        "                embedding_matrix[idx] = np.zeros(embedding_dim)\n",
        "\n",
        "    return embedding_matrix\n",
        "\n"
      ],
      "metadata": {
        "id": "9_o0ZGgaXLbm"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to load GloVe embeddings\n",
        "def load_glove_embeddings(glove_file_path):\n",
        "    embeddings_index = {}\n",
        "    with open(glove_file_path, 'r', encoding='utf-8') as f:\n",
        "        for line in f:\n",
        "            values = line.strip().split()\n",
        "            word = values[0]\n",
        "            vector = np.asarray(values[1:], dtype='float32')\n",
        "            embeddings_index[word] = vector\n",
        "    return embeddings_index"
      ],
      "metadata": {
        "id": "uzbnx6HrXLQH"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# TextDataset class for loading data\n",
        "class TextDataset(Dataset):\n",
        "    def __init__(self, dataset, vocabulary, word_to_index, oov_handling_method='unknown_token'):\n",
        "        self.dataset = dataset\n",
        "        self.vocabulary = vocabulary\n",
        "        self.word_to_index = word_to_index\n",
        "        self.oov_handling_method = oov_handling_method\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.dataset)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        sentence = self.dataset[idx]['text']\n",
        "        label = self.dataset[idx]['label']\n",
        "        tokens = word_tokenize(sentence.lower())\n",
        "\n",
        "        indices = []\n",
        "        for token in tokens:\n",
        "            if token in self.word_to_index:\n",
        "                indices.append(self.word_to_index[token])\n",
        "            else:\n",
        "                if self.oov_handling_method == 'unknown_token':\n",
        "                    indices.append(self.word_to_index[UNKNOWN_TOKEN])\n",
        "                elif self.oov_handling_method == 'random':\n",
        "                    # Assign a unique index to each OOV word\n",
        "                    if token not in self.word_to_index:\n",
        "                        self.word_to_index[token] = len(self.word_to_index)\n",
        "                    indices.append(self.word_to_index[token])\n",
        "                elif self.oov_handling_method == 'none':\n",
        "                    # Skip the word or handle as desired\n",
        "                    continue\n",
        "        return torch.tensor(indices, dtype=torch.long), torch.tensor(label, dtype=torch.long)\n"
      ],
      "metadata": {
        "id": "ymd6E3mqWzvM"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Custom collate functions\n",
        "def collate_fn(batch):\n",
        "    inputs, labels = zip(*batch)\n",
        "    inputs = [torch.tensor(x) for x in inputs]\n",
        "    padded_inputs = pad_sequence(inputs, batch_first=True, padding_value=0)  # Pad sequences\n",
        "    labels = torch.stack(labels)\n",
        "    return padded_inputs, labels\n",
        "\n",
        "def collate_fn_cnn(batch, max_length=100):\n",
        "    inputs, labels = zip(*batch)\n",
        "    inputs = [x[:max_length] if len(x) >= max_length else torch.cat([x, torch.zeros(max_length - len(x), dtype=torch.long)]) for x in inputs]\n",
        "    inputs = torch.stack(inputs)\n",
        "    labels = torch.tensor(labels)\n",
        "    return inputs, labels\n"
      ],
      "metadata": {
        "id": "OQSs-xzUW6wp"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# SentimentRNN class\n",
        "class SentimentRNN(nn.Module):\n",
        "    def __init__(self, embedding_matrix, hidden_size, output_size,\n",
        "                 rnn_type=\"RNN\", num_layers=1, use_bidirectional=False,\n",
        "                 use_dropout=False, use_batch_norm=False, use_layer_norm=False,\n",
        "                 aggregation_method='last_hidden', freeze_embeddings=True):\n",
        "        super(SentimentRNN, self).__init__()\n",
        "\n",
        "        vocab_size, embedding_dim = embedding_matrix.shape\n",
        "\n",
        "        # Embedding layer using pre-trained embeddings\n",
        "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
        "        self.embedding.weight = nn.Parameter(torch.tensor(embedding_matrix, dtype=torch.float32))\n",
        "        self.embedding.weight.requires_grad = not freeze_embeddings  # Control freezing\n",
        "\n",
        "        # Choose RNN type dynamically\n",
        "        if rnn_type == \"LSTM\":\n",
        "            self.rnn = nn.LSTM(embedding_dim, hidden_size, num_layers=num_layers,\n",
        "                               batch_first=True, bidirectional=use_bidirectional)\n",
        "        elif rnn_type == \"GRU\":\n",
        "            self.rnn = nn.GRU(embedding_dim, hidden_size, num_layers=num_layers,\n",
        "                              batch_first=True, bidirectional=use_bidirectional)\n",
        "        else:  # Default to Simple RNN\n",
        "            self.rnn = nn.RNN(embedding_dim, hidden_size, num_layers=num_layers,\n",
        "                              batch_first=True, bidirectional=use_bidirectional)\n",
        "\n",
        "        # Store the aggregation method\n",
        "        self.aggregation_method = aggregation_method\n",
        "\n",
        "        # Determine the final hidden size after aggregation\n",
        "        if aggregation_method == 'last_hidden':\n",
        "            if use_bidirectional:\n",
        "                final_hidden_size = hidden_size * 2\n",
        "            else:\n",
        "                final_hidden_size = hidden_size\n",
        "        else:\n",
        "            if use_bidirectional:\n",
        "                final_hidden_size = hidden_size * 2\n",
        "            else:\n",
        "                final_hidden_size = hidden_size\n",
        "\n",
        "        # Fully connected layer for classification\n",
        "        self.fc = nn.Linear(final_hidden_size, output_size)\n",
        "\n",
        "        # Optional Regularization Layers\n",
        "        self.use_dropout = use_dropout\n",
        "        self.use_batch_norm = use_batch_norm\n",
        "        self.use_layer_norm = use_layer_norm\n",
        "\n",
        "        if self.use_dropout:\n",
        "            self.dropout = nn.Dropout(0.3)  # Dropout rate of 0.3\n",
        "\n",
        "        if self.use_batch_norm:\n",
        "            self.batch_norm = nn.BatchNorm1d(final_hidden_size)\n",
        "\n",
        "        if self.use_layer_norm:\n",
        "            self.layer_norm = nn.LayerNorm(final_hidden_size)\n",
        "\n",
        "    def forward(self, x):\n",
        "        embedded = self.embedding(x)\n",
        "        output, hidden = self.rnn(embedded)\n",
        "\n",
        "        # For LSTM, hidden is a tuple of (h_n, c_n); use h_n\n",
        "        if isinstance(hidden, tuple):\n",
        "            hidden = hidden[0]  # h_n\n",
        "\n",
        "        if self.aggregation_method == 'last_hidden':\n",
        "            if self.rnn.bidirectional:\n",
        "                final_output = torch.cat((hidden[-2,:,:], hidden[-1,:,:]), dim=1)\n",
        "            else:\n",
        "                final_output = hidden[-1,:,:]\n",
        "        elif self.aggregation_method == 'mean_pooling':\n",
        "            final_output = output.mean(dim=1)\n",
        "        elif self.aggregation_method == 'max_pooling':\n",
        "            final_output, _ = torch.max(output, dim=1)\n",
        "        else:\n",
        "            raise ValueError(f\"Unknown aggregation method: {self.aggregation_method}\")\n",
        "\n",
        "        # Apply optional regularization layers\n",
        "        if self.use_batch_norm:\n",
        "            final_output = self.batch_norm(final_output)\n",
        "\n",
        "        if self.use_layer_norm:\n",
        "            final_output = self.layer_norm(final_output)\n",
        "\n",
        "        if self.use_dropout:\n",
        "            final_output = self.dropout(final_output)\n",
        "\n",
        "        return self.fc(final_output)"
      ],
      "metadata": {
        "id": "uEY6zYYKW6t1"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# SentimentCNN class\n",
        "class SentimentCNN(nn.Module):\n",
        "    def __init__(self, embedding_matrix, output_size, freeze_embeddings=True,\n",
        "                 num_filters=100, filter_sizes=[3,4,5], dropout_rate=0.5):\n",
        "        super(SentimentCNN, self).__init__()\n",
        "\n",
        "        vocab_size, embedding_dim = embedding_matrix.shape\n",
        "\n",
        "        # Embedding layer using pre-trained embeddings\n",
        "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
        "        self.embedding.weight = nn.Parameter(torch.tensor(embedding_matrix, dtype=torch.float32))\n",
        "        self.embedding.weight.requires_grad = not freeze_embeddings  # Control freezing\n",
        "\n",
        "        # Convolutional layers with multiple filter sizes\n",
        "        self.convs = nn.ModuleList([\n",
        "            nn.Conv2d(in_channels=1, out_channels=num_filters, kernel_size=(fs, embedding_dim))\n",
        "            for fs in filter_sizes\n",
        "        ])\n",
        "\n",
        "        # Fully connected layer\n",
        "        self.fc = nn.Linear(len(filter_sizes) * num_filters, output_size)\n",
        "\n",
        "        # Dropout layer\n",
        "        self.dropout = nn.Dropout(dropout_rate)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.embedding(x)  # Shape: (batch_size, seq_length, embedding_dim)\n",
        "        x = x.unsqueeze(1)  # Add channel dimension: (batch_size, 1, seq_length, embedding_dim)\n",
        "\n",
        "        # Apply convolution and ReLU activation\n",
        "        conv_outs = [torch.relu(conv(x)).squeeze(3) for conv in self.convs]\n",
        "\n",
        "        # Apply max pooling over the sequence length\n",
        "        pooled_outs = [torch.max(conv_out, dim=2)[0] for conv_out in conv_outs]\n",
        "\n",
        "        # Concatenate pooled outputs\n",
        "        cat = torch.cat(pooled_outs, dim=1)\n",
        "\n",
        "        # Apply dropout\n",
        "        out = self.dropout(cat)\n",
        "\n",
        "        # Fully connected layer\n",
        "        out = self.fc(out)\n",
        "        return out\n"
      ],
      "metadata": {
        "id": "enikXnhwW6rA"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to get optimizer\n",
        "def get_optimizer(params, model):\n",
        "    optimizer_type = params[\"optimizer_type\"]\n",
        "    lr = params[\"learning_rate\"]\n",
        "    weight_decay = params.get(\"weight_decay\", 0)  # Default to 0 if not specified\n",
        "\n",
        "    if optimizer_type == \"SGD\":\n",
        "        momentum = params.get(\"momentum\", 0)  # Default to 0 if not specified\n",
        "        return optim.SGD(model.parameters(), lr=lr, momentum=momentum, weight_decay=weight_decay)\n",
        "\n",
        "    elif optimizer_type == \"Adam\":\n",
        "        return optim.Adam(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
        "\n",
        "    else:\n",
        "        raise ValueError(f\"Unknown optimizer type: {optimizer_type}\")"
      ],
      "metadata": {
        "id": "zjircnwJW6oR"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to train the model\n",
        "def train_model(model, train_loader, valid_loader, test_loader, optimizer, epochs, patience, scheduler_step_size, scheduler_gamma, device):\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=scheduler_step_size, gamma=scheduler_gamma)\n",
        "\n",
        "    train_losses = []\n",
        "    val_accuracies = []\n",
        "    test_accuracies = []\n",
        "    best_val_accuracy = 0\n",
        "    epochs_no_improve = 0\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        model.train()\n",
        "        running_loss = 0.0\n",
        "        for inputs, labels in train_loader:\n",
        "            inputs, labels = inputs.to(device), labels.to(device)\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(inputs)\n",
        "            loss = criterion(outputs, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            running_loss += loss.item()\n",
        "\n",
        "        avg_train_loss = running_loss / len(train_loader)\n",
        "        train_losses.append(avg_train_loss)\n",
        "\n",
        "        # Validation\n",
        "        val_accuracy = evaluate_accuracy(model, valid_loader, device)\n",
        "        val_accuracies.append(val_accuracy)\n",
        "\n",
        "        print(f'Epoch {epoch+1}, Loss: {avg_train_loss:.4f}, Validation Accuracy: {val_accuracy:.2f}%')\n",
        "\n",
        "        # Early stopping logic\n",
        "        if val_accuracy > best_val_accuracy:\n",
        "            best_val_accuracy = val_accuracy\n",
        "            epochs_no_improve = 0\n",
        "            # Save the best model\n",
        "            torch.save(model.state_dict(), f'best_model_{epoch+1}.pt')\n",
        "        else:\n",
        "            epochs_no_improve += 1\n",
        "\n",
        "        if epochs_no_improve >= patience:\n",
        "            print(f\"Early stopping at epoch {epoch + 1}\")\n",
        "            break\n",
        "\n",
        "        scheduler.step()\n",
        "\n",
        "    # Load the best model before evaluating on test set\n",
        "    best_epoch = val_accuracies.index(best_val_accuracy) + 1\n",
        "    model.load_state_dict(torch.load(f'best_model_{best_epoch}.pt'))\n",
        "\n",
        "    # Test Accuracy\n",
        "    test_accuracy = evaluate_accuracy(model, test_loader, device)\n",
        "    test_accuracies = [test_accuracy] * len(train_losses)\n",
        "    print(f'Test Accuracy: {test_accuracy:.2f}%')\n",
        "\n",
        "    return train_losses, val_accuracies, test_accuracies"
      ],
      "metadata": {
        "id": "7jvCWISIW6im"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to evaluate accuracy\n",
        "def evaluate_accuracy(model, data_loader, device):\n",
        "    model.eval()\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    with torch.no_grad():\n",
        "        for inputs, labels in data_loader:\n",
        "            inputs, labels = inputs.to(device), labels.to(device)\n",
        "            outputs = model(inputs)\n",
        "            _, predicted = outputs.max(1)\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "    accuracy = 100 * correct / total\n",
        "    return accuracy\n"
      ],
      "metadata": {
        "id": "4dvJWIl8W6Z5"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to run experiments from CSV\n",
        "def run_experiments_from_csv(params_csv, train_dataset, validation_dataset, test_dataset):\n",
        "    csv_filename = 'training_results.csv'\n",
        "    fieldnames = ['Epoch', 'Train Loss', 'Validation Accuracy', 'Test Accuracy',\n",
        "                  'model_type', 'rnn_type', 'num_layers', 'bidirectional', 'dropout', 'batch_norm', 'layer_norm',\n",
        "                  'aggregation_method', 'optimizer_type', 'learning_rate', 'momentum', 'weight_decay',\n",
        "                  'batch_size', 'epochs', 'patience', 'hidden_size', 'output_size',\n",
        "                  'freeze_embeddings', 'num_filters', 'filter_sizes', 'dropout_rate', 'oov_handling_method',\n",
        "                  'embedding_dim', 'glove_file_path']\n",
        "\n",
        "    # Open the CSV file with the configurations\n",
        "    with open(params_csv, mode='r') as params_file:\n",
        "        reader = csv.DictReader(params_file)\n",
        "\n",
        "        # Prepare the results CSV to store the results for each run\n",
        "        with open(csv_filename, mode='w', newline='') as result_file:\n",
        "            writer = csv.DictWriter(result_file, fieldnames=fieldnames)\n",
        "            writer.writeheader()\n",
        "\n",
        "            for config in reader:\n",
        "                # Convert necessary parameters to appropriate types\n",
        "                config['num_layers'] = int(config['num_layers']) if config['num_layers'] != 'N/A' else None\n",
        "                config['use_bidirectional'] = config['use_bidirectional'] == 'True' if config['use_bidirectional'] != 'N/A' else None\n",
        "                config['use_dropout'] = config['use_dropout'] == 'True'\n",
        "                config['use_batch_norm'] = config['use_batch_norm'] == 'True'\n",
        "                config['use_layer_norm'] = config['use_layer_norm'] == 'True'\n",
        "                config['batch_size'] = int(config['batch_size'])\n",
        "                config['epochs'] = int(config['epochs'])\n",
        "                config['patience'] = int(config['patience'])\n",
        "                config['hidden_size'] = int(config['hidden_size']) if config['hidden_size'] != 'N/A' else None\n",
        "                config['output_size'] = int(config['output_size'])\n",
        "                config['learning_rate'] = float(config['learning_rate'])\n",
        "                config['momentum'] = float(config['momentum'])\n",
        "                config['weight_decay'] = float(config['weight_decay'])\n",
        "                config['freeze_embeddings'] = config['freeze_embeddings'] == 'True'\n",
        "                config['num_filters'] = int(config['num_filters']) if config['num_filters'] != 'N/A' else None\n",
        "                config['filter_sizes'] = [int(fs) for fs in config['filter_sizes'].split(',')] if config['filter_sizes'] != 'N/A' else None\n",
        "                config['dropout_rate'] = float(config['dropout_rate']) if config['dropout_rate'] != 'N/A' else None\n",
        "                config['aggregation_method'] = config['aggregation_method']\n",
        "                config['oov_handling_method'] = config['oov_handling_method']\n",
        "                config['embedding_dim'] = int(config['embedding_dim'])\n",
        "                config['glove_file_path'] = config['glove_file_path']\n",
        "\n",
        "                # Load GloVe embeddings\n",
        "                glove_embeddings = load_glove_embeddings(config['glove_file_path'])\n",
        "\n",
        "                # Build vocabulary and create embedding matrix\n",
        "                vocabulary = build_vocabulary(train_dataset, config['oov_handling_method'])\n",
        "                embedding_matrix = create_embedding_matrix(config['embedding_dim'], vocabulary, glove_embeddings, config['oov_handling_method'])\n",
        "\n",
        "                # Build word_to_index mapping\n",
        "                word_to_index = {word: idx for idx, word in enumerate(vocabulary)}\n",
        "\n",
        "                # Initialize the model based on 'model_type'\n",
        "                if config['model_type'] == 'RNN':\n",
        "                    model = SentimentRNN(\n",
        "                        embedding_matrix=embedding_matrix,\n",
        "                        hidden_size=config['hidden_size'],\n",
        "                        output_size=config['output_size'],\n",
        "                        rnn_type=config['rnn_type'],\n",
        "                        num_layers=config['num_layers'],\n",
        "                        use_bidirectional=config['use_bidirectional'],\n",
        "                        use_dropout=config['use_dropout'],\n",
        "                        use_batch_norm=config['use_batch_norm'],\n",
        "                        use_layer_norm=config['use_layer_norm'],\n",
        "                        aggregation_method=config['aggregation_method'],\n",
        "                        freeze_embeddings=config['freeze_embeddings']\n",
        "                    )\n",
        "                    collate_function = collate_fn\n",
        "                elif config['model_type'] == 'CNN':\n",
        "                    model = SentimentCNN(\n",
        "                        embedding_matrix=embedding_matrix,\n",
        "                        output_size=config['output_size'],\n",
        "                        freeze_embeddings=config['freeze_embeddings'],\n",
        "                        num_filters=config['num_filters'],\n",
        "                        filter_sizes=config['filter_sizes'],\n",
        "                        dropout_rate=config['dropout_rate']\n",
        "                    )\n",
        "                    collate_function = collate_fn_cnn\n",
        "                else:\n",
        "                    raise ValueError(f\"Unknown model type: {config['model_type']}\")\n",
        "\n",
        "                # Get the optimizer dynamically based on the config\n",
        "                optimizer_params = {\n",
        "                    'optimizer_type': config['optimizer_type'],\n",
        "                    'learning_rate': config['learning_rate'],\n",
        "                    'momentum': config['momentum'],\n",
        "                    'weight_decay': config['weight_decay']\n",
        "                }\n",
        "                optimizer = get_optimizer(optimizer_params, model)\n",
        "\n",
        "                # Create DataLoaders using the appropriate collate function\n",
        "                train_loader = DataLoader(TextDataset(train_dataset, vocabulary, word_to_index, config['oov_handling_method']), batch_size=config['batch_size'], shuffle=True, collate_fn=collate_function)\n",
        "                valid_loader = DataLoader(TextDataset(validation_dataset, vocabulary, word_to_index, config['oov_handling_method']), batch_size=config['batch_size'], collate_fn=collate_function)\n",
        "                test_loader = DataLoader(TextDataset(test_dataset, vocabulary, word_to_index, config['oov_handling_method']), batch_size=config['batch_size'], collate_fn=collate_function)\n",
        "\n",
        "                device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "                model.to(device)\n",
        "\n",
        "                # Train the model\n",
        "                train_loss, val_accuracy, test_accuracy = train_model(\n",
        "                    model=model,\n",
        "                    train_loader=train_loader,\n",
        "                    valid_loader=valid_loader,\n",
        "                    test_loader=test_loader,\n",
        "                    optimizer=optimizer,\n",
        "                    epochs=config['epochs'],\n",
        "                    patience=config['patience'],\n",
        "                    scheduler_step_size=3,\n",
        "                    scheduler_gamma=0.1,\n",
        "                    device=device\n",
        "                )\n",
        "\n",
        "                # Log results to CSV after training completes\n",
        "                for epoch in range(len(train_loss)):  # Iterate through each epoch's results\n",
        "                    writer.writerow({\n",
        "                        'Epoch': epoch + 1,\n",
        "                        'Train Loss': train_loss[epoch],\n",
        "                        'Validation Accuracy': val_accuracy[epoch],\n",
        "                        'Test Accuracy': test_accuracy[epoch],\n",
        "                        'model_type': config['model_type'],\n",
        "                        'rnn_type': config['rnn_type'],\n",
        "                        'num_layers': config['num_layers'],\n",
        "                        'bidirectional': config['use_bidirectional'],\n",
        "                        'dropout': config['use_dropout'],\n",
        "                        'batch_norm': config['use_batch_norm'],\n",
        "                        'layer_norm': config['use_layer_norm'],\n",
        "                        'aggregation_method': config['aggregation_method'],\n",
        "                        'optimizer_type': config['optimizer_type'],\n",
        "                        'learning_rate': config['learning_rate'],\n",
        "                        'momentum': config['momentum'],\n",
        "                        'weight_decay': config['weight_decay'],\n",
        "                        'batch_size': config['batch_size'],\n",
        "                        'epochs': config['epochs'],\n",
        "                        'patience': config['patience'],\n",
        "                        'hidden_size': config['hidden_size'],\n",
        "                        'output_size': config['output_size'],\n",
        "                        'freeze_embeddings': config['freeze_embeddings'],\n",
        "                        'num_filters': config['num_filters'],\n",
        "                        'filter_sizes': config['filter_sizes'],\n",
        "                        'dropout_rate': config['dropout_rate'],\n",
        "                        'oov_handling_method': config['oov_handling_method'],\n",
        "                        'embedding_dim': config['embedding_dim'],\n",
        "                        'glove_file_path': config['glove_file_path']\n",
        "                    })\n",
        "\n",
        "    print(f\"Experiments completed. Results saved to {csv_filename}.\")"
      ],
      "metadata": {
        "id": "t4_SryycXG54"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# # To download experiment params from gdrive (not used now, use the create csv below for colab env)\n",
        "# experiment_param_file_id = '1NsO-vQyC_CJjykdd3gbea1CdzHT5pKyY'\n",
        "# experiment_param_file = 'experiment_params.csv'\n",
        "# !gdown {experiment_param_file_id} -O {experiment_param_file}"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G6apxqGT4xMR",
        "outputId": "e97b208f-4f02-46c5-86bc-62f53723173c"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1NsO-vQyC_CJjykdd3gbea1CdzHT5pKyY\n",
            "To: /content/experiment_params.csv\n",
            "\r  0% 0.00/449 [00:00<?, ?B/s]\r100% 449/449 [00:00<00:00, 1.48MB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to create CSV in colab environment\n",
        "\n",
        "import csv\n",
        "\n",
        "# String data in CSV format\n",
        "header = [\n",
        "    \"model_type\",          # Possible Values: 'RNN', 'CNN'                                  | Meaning: Specifies the type of model to use.                    | Type: str\n",
        "    \"rnn_type\",            # Possible Values: 'RNN', 'LSTM', 'GRU'                          | Meaning: The type of recurrent layer to use in the RNN.         | Type: str\n",
        "    \"num_layers\",          # Possible Values: Positive integers (e.g., 1, 2)                | Meaning: The number of layers in the RNN.                       | Type: int\n",
        "    \"use_bidirectional\",   # Possible Values: True, False                                   | Meaning: Whether to use a bidirectional RNN.                    | Type: bool\n",
        "    \"use_dropout\",         # Possible Values: True, False                                   | Meaning: Whether to apply dropout regularization in the model.  | Type: bool\n",
        "    \"use_batch_norm\",      # Possible Values: True, False                                   | Meaning: Whether to apply batch normalization.                  | Type: bool\n",
        "    \"use_layer_norm\",      # Possible Values: True, False                                   | Meaning: Whether to apply layer normalization.                  | Type: bool\n",
        "    \"aggregation_method\",  # Possible Values: 'last_hidden', 'mean_pooling', 'max_pooling'  | Meaning: Method to aggregate the sequence of hidden states.     | Type: str\n",
        "    \"optimizer_type\",      # Possible Values: 'SGD', 'Adam'                                 | Meaning: The optimizer to use.                                  | Type: str\n",
        "    \"learning_rate\",       # Possible Values: Positive floats (e.g., 0.01)                  | Meaning: Learning rate for the optimizer.                       | Type: float\n",
        "    \"momentum\",            # Possible Values: Floats between 0 and 1 (e.g., 0.9)            | Meaning: Momentum factor (only for 'SGD').                      | Type: float\n",
        "    \"weight_decay\",        # Possible Values: Non-negative floats (e.g., 0.0001)            | Meaning: Weight decay (L2 regularization).                      | Type: float\n",
        "    \"batch_size\",          # Possible Values: Positive integers (e.g., 32)                  | Meaning: Batch size during training.                            | Type: int\n",
        "    \"epochs\",              # Possible Values: Positive integers (e.g., 20)                  | Meaning: Maximum number of training epochs.                     | Type: int\n",
        "    \"patience\",            # Possible Values: Positive integers (e.g., 3)                   | Meaning: Number of epochs with no improvement before stopping.  | Type: int\n",
        "    \"hidden_size\",         # Possible Values: Positive integers (e.g., 128)                 | Meaning: Number of features in the hidden state.                | Type: int\n",
        "    \"output_size\",         # Possible Values: Positive integers (e.g., 2)                   | Meaning: Number of output classes.                              | Type: int\n",
        "    \"freeze_embeddings\",   # Possible Values: True, False                                   | Meaning: Whether to freeze the embedding layer weights.         | Type: bool\n",
        "    \"num_filters\",         # Possible Values: Positive integers (e.g., 100)                 | Meaning: Number of filters in the CNN.                          | Type: int\n",
        "    \"filter_sizes\",        # Possible Values: List of positive integers (e.g., [3,4,5])     | Meaning: Sizes of convolutional filters.                        | Type: list of int\n",
        "    \"dropout_rate\",        # Possible Values: Floats between 0 and 1 (e.g., 0.5)            | Meaning: Dropout rate after convolutional layers.               | Type: float\n",
        "    \"oov_handling_method\", # Possible Values: 'unknown_token', 'random', 'none'             | Meaning: Strategy to handle OOV words.                          | Type: str\n",
        "    \"embedding_dim\",       # Possible Values: Positive integers (e.g., 50, 100, 200, 300)   | Meaning: Dimension of word embeddings.                          | Type: int\n",
        "    \"glove_file_path\"      # Possible Values: String (file path)                            | Meaning: Path to the GloVe embedding file.                      | Type: str\n",
        "]\n",
        "\n",
        "\n",
        "data = [\n",
        "    [\n",
        "        \"RNN\", \"RNN\", 1, False, False, False, False, \"last_hidden\", \"Adam\", 0.01, 0, 0.0001, 32, 100, 3, 128, 2, True, 0, 0, 0, \"none\", 100, \"glove.6B.100d.txt\"\n",
        "    ]\n",
        "]\n",
        "\n",
        "# Define the filename\n",
        "filename = \"experiment_params.csv\"\n",
        "\n",
        "# Write the data to a CSV file\n",
        "with open(filename, mode='w', newline='') as file:\n",
        "    writer = csv.writer(file)\n",
        "    writer.writerow(header)\n",
        "    writer.writerows(data)"
      ],
      "metadata": {
        "id": "g651a7qch7vn"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "run_experiments_from_csv(experiment_param_file, train_dataset, validation_dataset, test_dataset)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5xFaeosRZRSQ",
        "outputId": "260e9e8c-783d-4775-9e90-217e00161c3e"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-15-eb9525725bc2>:4: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  inputs = [torch.tensor(x) for x in inputs]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1, Loss: 0.7485, Validation Accuracy: 50.00%\n",
            "Epoch 2, Loss: 0.7225, Validation Accuracy: 50.00%\n",
            "Epoch 3, Loss: 0.7304, Validation Accuracy: 50.00%\n",
            "Epoch 4, Loss: 0.6958, Validation Accuracy: 50.09%\n",
            "Epoch 5, Loss: 0.6977, Validation Accuracy: 50.00%\n",
            "Epoch 6, Loss: 0.6975, Validation Accuracy: 50.00%\n",
            "Epoch 7, Loss: 0.6937, Validation Accuracy: 50.09%\n",
            "Early stopping at epoch 7\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-19-fd412c873766>:50: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  model.load_state_dict(torch.load(f'best_model_{best_epoch}.pt'))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Accuracy: 50.00%\n",
            "Experiments completed. Results saved to training_results.csv.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "r1BNKjKJmco_"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}