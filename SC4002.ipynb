{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Abeszz/SC4002-NLP-Assignment/blob/main/SC4002.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fRPGfwCv_acw"
      },
      "source": [
        "# Installation & Requirements :"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "J81YDyhL_mX5",
        "outputId": "9fc684bb-e410-484f-b372-bcef54d9ba95"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[33mDEPRECATION: Configuring installation scheme with distutils config files is deprecated and will no longer work in the near future. If you are using a Homebrew or Linuxbrew Python, please see discussion at https://github.com/Homebrew/homebrew-core/issues/76621\u001b[0m\u001b[33m\n",
            "\u001b[0mRequirement already satisfied: datasets in /opt/homebrew/lib/python3.9/site-packages (3.0.2)\n",
            "Requirement already satisfied: filelock in /opt/homebrew/lib/python3.9/site-packages (from datasets) (3.12.4)\n",
            "Requirement already satisfied: numpy>=1.17 in /opt/homebrew/lib/python3.9/site-packages (from datasets) (1.25.2)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /opt/homebrew/lib/python3.9/site-packages (from datasets) (17.0.0)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /opt/homebrew/lib/python3.9/site-packages (from datasets) (0.3.8)\n",
            "Requirement already satisfied: pandas in /opt/homebrew/lib/python3.9/site-packages (from datasets) (2.0.3)\n",
            "Requirement already satisfied: requests>=2.32.2 in /opt/homebrew/lib/python3.9/site-packages (from datasets) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.66.3 in /opt/homebrew/lib/python3.9/site-packages (from datasets) (4.66.5)\n",
            "Requirement already satisfied: xxhash in /opt/homebrew/lib/python3.9/site-packages (from datasets) (3.5.0)\n",
            "Requirement already satisfied: multiprocess<0.70.17 in /opt/homebrew/lib/python3.9/site-packages (from datasets) (0.70.16)\n",
            "Requirement already satisfied: fsspec<=2024.9.0,>=2023.1.0 in /opt/homebrew/lib/python3.9/site-packages (from fsspec[http]<=2024.9.0,>=2023.1.0->datasets) (2023.9.2)\n",
            "Requirement already satisfied: aiohttp in /opt/homebrew/lib/python3.9/site-packages (from datasets) (3.8.5)\n",
            "Requirement already satisfied: huggingface-hub>=0.23.0 in /opt/homebrew/lib/python3.9/site-packages (from datasets) (0.26.1)\n",
            "Requirement already satisfied: packaging in /opt/homebrew/lib/python3.9/site-packages (from datasets) (23.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /opt/homebrew/lib/python3.9/site-packages (from datasets) (6.0.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /opt/homebrew/lib/python3.9/site-packages (from aiohttp->datasets) (24.2.0)\n",
            "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in /opt/homebrew/lib/python3.9/site-packages (from aiohttp->datasets) (3.2.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /opt/homebrew/lib/python3.9/site-packages (from aiohttp->datasets) (6.0.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /opt/homebrew/lib/python3.9/site-packages (from aiohttp->datasets) (4.0.3)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /opt/homebrew/lib/python3.9/site-packages (from aiohttp->datasets) (1.9.2)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /opt/homebrew/lib/python3.9/site-packages (from aiohttp->datasets) (1.4.0)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /opt/homebrew/lib/python3.9/site-packages (from aiohttp->datasets) (1.3.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/homebrew/lib/python3.9/site-packages (from huggingface-hub>=0.23.0->datasets) (4.12.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /opt/homebrew/lib/python3.9/site-packages (from requests>=2.32.2->datasets) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/homebrew/lib/python3.9/site-packages (from requests>=2.32.2->datasets) (2.0.5)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /opt/homebrew/lib/python3.9/site-packages (from requests>=2.32.2->datasets) (2023.7.22)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /opt/homebrew/lib/python3.9/site-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /opt/homebrew/lib/python3.9/site-packages (from pandas->datasets) (2023.3)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /opt/homebrew/lib/python3.9/site-packages (from pandas->datasets) (2023.3)\n",
            "Requirement already satisfied: six>=1.5 in /opt/homebrew/lib/python3.9/site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n",
            "\u001b[33mDEPRECATION: Configuring installation scheme with distutils config files is deprecated and will no longer work in the near future. If you are using a Homebrew or Linuxbrew Python, please see discussion at https://github.com/Homebrew/homebrew-core/issues/76621\u001b[0m\u001b[33m\n",
            "\u001b[0m\n",
            "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.0\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.2\u001b[0m\n",
            "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython3.9 -m pip install --upgrade pip\u001b[0m\n",
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        }
      ],
      "source": [
        "pip install datasets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "ZZ7IipO2Bmo1",
        "outputId": "ad2cb880-8c6f-455d-9418-45f76a69e52e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[33mDEPRECATION: Configuring installation scheme with distutils config files is deprecated and will no longer work in the near future. If you are using a Homebrew or Linuxbrew Python, please see discussion at https://github.com/Homebrew/homebrew-core/issues/76621\u001b[0m\u001b[33m\n",
            "\u001b[0mRequirement already satisfied: nltk in /opt/homebrew/lib/python3.9/site-packages (3.8.1)\n",
            "Requirement already satisfied: click in /opt/homebrew/lib/python3.9/site-packages (from nltk) (8.1.7)\n",
            "Requirement already satisfied: joblib in /opt/homebrew/lib/python3.9/site-packages (from nltk) (1.3.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /opt/homebrew/lib/python3.9/site-packages (from nltk) (2023.8.8)\n",
            "Requirement already satisfied: tqdm in /opt/homebrew/lib/python3.9/site-packages (from nltk) (4.66.5)\n",
            "\u001b[33mDEPRECATION: Configuring installation scheme with distutils config files is deprecated and will no longer work in the near future. If you are using a Homebrew or Linuxbrew Python, please see discussion at https://github.com/Homebrew/homebrew-core/issues/76621\u001b[0m\u001b[33m\n",
            "\u001b[0m\n",
            "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.0\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.2\u001b[0m\n",
            "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython3.9 -m pip install --upgrade pip\u001b[0m\n",
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        }
      ],
      "source": [
        "pip install nltk"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "wzYNgiwEBtoL",
        "outputId": "c92815ec-80c8-4c93-9a1d-c84e2810aa97"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[33mDEPRECATION: Configuring installation scheme with distutils config files is deprecated and will no longer work in the near future. If you are using a Homebrew or Linuxbrew Python, please see discussion at https://github.com/Homebrew/homebrew-core/issues/76621\u001b[0m\u001b[33m\n",
            "\u001b[0mRequirement already satisfied: numpy in /opt/homebrew/lib/python3.9/site-packages (1.25.2)\n",
            "\u001b[33mDEPRECATION: Configuring installation scheme with distutils config files is deprecated and will no longer work in the near future. If you are using a Homebrew or Linuxbrew Python, please see discussion at https://github.com/Homebrew/homebrew-core/issues/76621\u001b[0m\u001b[33m\n",
            "\u001b[0m\n",
            "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.0\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.2\u001b[0m\n",
            "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython3.9 -m pip install --upgrade pip\u001b[0m\n",
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        }
      ],
      "source": [
        "pip install numpy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "3pFMpM5qpLFn",
        "outputId": "638d9c8d-5d98-420b-8a33-945d350d1d4a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[33mDEPRECATION: Configuring installation scheme with distutils config files is deprecated and will no longer work in the near future. If you are using a Homebrew or Linuxbrew Python, please see discussion at https://github.com/Homebrew/homebrew-core/issues/76621\u001b[0m\u001b[33m\n",
            "\u001b[0mRequirement already satisfied: npm in /opt/homebrew/lib/python3.9/site-packages (0.1.1)\n",
            "Requirement already satisfied: optional-django==0.1.0 in /opt/homebrew/lib/python3.9/site-packages (from npm) (0.1.0)\n",
            "\u001b[33mDEPRECATION: Configuring installation scheme with distutils config files is deprecated and will no longer work in the near future. If you are using a Homebrew or Linuxbrew Python, please see discussion at https://github.com/Homebrew/homebrew-core/issues/76621\u001b[0m\u001b[33m\n",
            "\u001b[0m\n",
            "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.0\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.2\u001b[0m\n",
            "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython3.9 -m pip install --upgrade pip\u001b[0m\n",
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        }
      ],
      "source": [
        "pip install npm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "0nqNXGOPywf8",
        "outputId": "c13c2fea-9ddc-4591-8271-ede14fc71776"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[33mDEPRECATION: Configuring installation scheme with distutils config files is deprecated and will no longer work in the near future. If you are using a Homebrew or Linuxbrew Python, please see discussion at https://github.com/Homebrew/homebrew-core/issues/76621\u001b[0m\u001b[33m\n",
            "\u001b[0mRequirement already satisfied: torch in /opt/homebrew/lib/python3.9/site-packages (2.3.0)\n",
            "Requirement already satisfied: filelock in /opt/homebrew/lib/python3.9/site-packages (from torch) (3.12.4)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /opt/homebrew/lib/python3.9/site-packages (from torch) (4.12.2)\n",
            "Requirement already satisfied: sympy in /opt/homebrew/lib/python3.9/site-packages (from torch) (1.12)\n",
            "Requirement already satisfied: networkx in /opt/homebrew/lib/python3.9/site-packages (from torch) (3.1)\n",
            "Requirement already satisfied: jinja2 in /opt/homebrew/lib/python3.9/site-packages (from torch) (3.1.2)\n",
            "Requirement already satisfied: fsspec in /opt/homebrew/lib/python3.9/site-packages (from torch) (2023.9.2)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /opt/homebrew/lib/python3.9/site-packages (from jinja2->torch) (2.1.3)\n",
            "Requirement already satisfied: mpmath>=0.19 in /opt/homebrew/lib/python3.9/site-packages (from sympy->torch) (1.3.0)\n",
            "\u001b[33mDEPRECATION: Configuring installation scheme with distutils config files is deprecated and will no longer work in the near future. If you are using a Homebrew or Linuxbrew Python, please see discussion at https://github.com/Homebrew/homebrew-core/issues/76621\u001b[0m\u001b[33m\n",
            "\u001b[0m\n",
            "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.0\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.2\u001b[0m\n",
            "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython3.9 -m pip install --upgrade pip\u001b[0m\n",
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        }
      ],
      "source": [
        "pip install torch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[33mDEPRECATION: Configuring installation scheme with distutils config files is deprecated and will no longer work in the near future. If you are using a Homebrew or Linuxbrew Python, please see discussion at https://github.com/Homebrew/homebrew-core/issues/76621\u001b[0m\u001b[33m\n",
            "\u001b[0mRequirement already satisfied: gdown in /opt/homebrew/lib/python3.9/site-packages (5.2.0)\n",
            "Requirement already satisfied: beautifulsoup4 in /opt/homebrew/lib/python3.9/site-packages (from gdown) (4.12.3)\n",
            "Requirement already satisfied: filelock in /opt/homebrew/lib/python3.9/site-packages (from gdown) (3.12.4)\n",
            "Requirement already satisfied: requests[socks] in /opt/homebrew/lib/python3.9/site-packages (from gdown) (2.32.3)\n",
            "Requirement already satisfied: tqdm in /opt/homebrew/lib/python3.9/site-packages (from gdown) (4.66.5)\n",
            "Requirement already satisfied: soupsieve>1.2 in /opt/homebrew/lib/python3.9/site-packages (from beautifulsoup4->gdown) (2.6)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/homebrew/lib/python3.9/site-packages (from requests[socks]->gdown) (3.2.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /opt/homebrew/lib/python3.9/site-packages (from requests[socks]->gdown) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/homebrew/lib/python3.9/site-packages (from requests[socks]->gdown) (2.0.5)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /opt/homebrew/lib/python3.9/site-packages (from requests[socks]->gdown) (2023.7.22)\n",
            "Requirement already satisfied: PySocks!=1.5.7,>=1.5.6 in /opt/homebrew/lib/python3.9/site-packages (from requests[socks]->gdown) (1.7.1)\n",
            "\u001b[33mDEPRECATION: Configuring installation scheme with distutils config files is deprecated and will no longer work in the near future. If you are using a Homebrew or Linuxbrew Python, please see discussion at https://github.com/Homebrew/homebrew-core/issues/76621\u001b[0m\u001b[33m\n",
            "\u001b[0m\n",
            "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.0\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.2\u001b[0m\n",
            "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython3.9 -m pip install --upgrade pip\u001b[0m\n",
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        }
      ],
      "source": [
        "pip install gdown"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "IFIJIoS9DuVa",
        "outputId": "4f1b709f-e539-43ef-8641-d723e2cfcfd8"
      },
      "outputs": [],
      "source": [
        "# glove_file_id = '17CUd7jxuh6ptIljKaz_9gJQ8-40JXJ-F'\n",
        "# glove_file = 'glove.6B.100d.txt'\n",
        "# !gdown {glove_file_id} -O {glove_file}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "B8qRXCT2GpKd",
        "outputId": "4aaf10a4-8bcb-404e-f395-c1a84191c940"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /opt/homebrew/opt/python@3.9/\n",
            "[nltk_data]     Frameworks/Python.framework/Versions/3.9/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package punkt_tab to /opt/homebrew/opt/python@\n",
            "[nltk_data]     3.9/Frameworks/Python.framework/Versions/3.9/nltk_data\n",
            "[nltk_data]     ...\n",
            "[nltk_data]   Package punkt_tab is already up-to-date!\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 8,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import os\n",
        "import nltk\n",
        "import sys\n",
        "\n",
        "env_base_path = sys.prefix\n",
        "nltk_path = os.path.join(env_base_path, 'nltk_data')\n",
        "nltk.download('punkt', nltk_path)\n",
        "nltk.download('punkt_tab', nltk_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iBA7sNlf9hW5",
        "outputId": "99cdcdfb-e1a5-48b7-e402-112617a27eb2"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/opt/homebrew/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
            "  from .autonotebook import tqdm as notebook_tqdm\n"
          ]
        }
      ],
      "source": [
        "from datasets import load_dataset\n",
        "dataset = load_dataset('rotten_tomatoes')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "o_TvlwmgB35t"
      },
      "outputs": [],
      "source": [
        "train_dataset = dataset['train']\n",
        "validation_dataset = dataset['validation']\n",
        "test_dataset = dataset['test']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "cBY500zfFszo"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "import numpy as np\n",
        "from nltk.tokenize import word_tokenize\n",
        "from collections import Counter\n",
        "import csv"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "rDD4AHuqWu_h"
      },
      "outputs": [],
      "source": [
        "# Global variables\n",
        "UNKNOWN_TOKEN = '<UNKNOWN>'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "9_o0ZGgaXLbm"
      },
      "outputs": [],
      "source": [
        "# Functions to build vocabulary and create embedding matrix\n",
        "def build_vocabulary(dataset, oov_handling_method='unknown_token'):\n",
        "    vocab_counter = Counter()\n",
        "    for sample in dataset:\n",
        "        tokens = word_tokenize(sample['text'].lower())\n",
        "        vocab_counter.update(tokens)\n",
        "    vocabulary = list(vocab_counter.keys())\n",
        "    if oov_handling_method == 'unknown_token':\n",
        "        if UNKNOWN_TOKEN not in vocabulary:\n",
        "            vocabulary.append(UNKNOWN_TOKEN)\n",
        "    return vocabulary\n",
        "\n",
        "def create_embedding_matrix(embedding_dim, vocabulary, glove_embeddings, oov_handling_method='unknown_token'):\n",
        "    vocab_size = len(vocabulary)\n",
        "    embedding_matrix = np.zeros((vocab_size, embedding_dim))\n",
        "    word_to_index = {word: idx for idx, word in enumerate(vocabulary)}\n",
        "\n",
        "    # Initialize special embeddings\n",
        "    if oov_handling_method == 'unknown_token':\n",
        "        # Use a single <UNKNOWN> token for all OOV words\n",
        "        unknown_vector = np.random.normal(scale=0.6, size=(embedding_dim,))\n",
        "        unknown_index = word_to_index[UNKNOWN_TOKEN]\n",
        "        embedding_matrix[unknown_index] = unknown_vector\n",
        "\n",
        "    # Fill the embedding matrix\n",
        "    for word, idx in word_to_index.items():\n",
        "        if word in glove_embeddings:\n",
        "            embedding_matrix[idx] = glove_embeddings[word]\n",
        "        else:\n",
        "            if oov_handling_method == 'unknown_token':\n",
        "                embedding_matrix[idx] = embedding_matrix[unknown_index]\n",
        "            elif oov_handling_method == 'random':\n",
        "                embedding_matrix[idx] = np.random.normal(scale=0.6, size=(embedding_dim,))\n",
        "            elif oov_handling_method == 'none':\n",
        "                embedding_matrix[idx] = np.zeros(embedding_dim)\n",
        "\n",
        "    return embedding_matrix\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "uzbnx6HrXLQH"
      },
      "outputs": [],
      "source": [
        "# Function to load GloVe embeddings\n",
        "def load_glove_embeddings(glove_file_path):\n",
        "    embeddings_index = {}\n",
        "    with open(glove_file_path, 'r', encoding='utf-8') as f:\n",
        "        for line in f:\n",
        "            values = line.strip().split()\n",
        "            word = values[0]\n",
        "            vector = np.asarray(values[1:], dtype='float32')\n",
        "            embeddings_index[word] = vector\n",
        "    return embeddings_index"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "ymd6E3mqWzvM"
      },
      "outputs": [],
      "source": [
        "# TextDataset class for loading data\n",
        "class TextDataset(Dataset):\n",
        "    def __init__(self, dataset, vocabulary, word_to_index, oov_handling_method='unknown_token'):\n",
        "        self.dataset = dataset\n",
        "        self.vocabulary = vocabulary\n",
        "        self.word_to_index = word_to_index\n",
        "        self.oov_handling_method = oov_handling_method\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.dataset)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        sentence = self.dataset[idx]['text']\n",
        "        label = self.dataset[idx]['label']\n",
        "        tokens = word_tokenize(sentence.lower())\n",
        "\n",
        "        indices = []\n",
        "        for token in tokens:\n",
        "            if token in self.word_to_index:\n",
        "                indices.append(self.word_to_index[token])\n",
        "            else:\n",
        "                if self.oov_handling_method == 'unknown_token':\n",
        "                    indices.append(self.word_to_index[UNKNOWN_TOKEN])\n",
        "                elif self.oov_handling_method == 'random':\n",
        "                    # Assign a unique index to each OOV word\n",
        "                    if token not in self.word_to_index:\n",
        "                        self.word_to_index[token] = len(self.word_to_index)\n",
        "                    indices.append(self.word_to_index[token])\n",
        "                elif self.oov_handling_method == 'none':\n",
        "                    # Skip the word or handle as desired\n",
        "                    continue\n",
        "        return torch.tensor(indices, dtype=torch.long), torch.tensor(label, dtype=torch.long)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "OQSs-xzUW6wp"
      },
      "outputs": [],
      "source": [
        "# Custom collate functions\n",
        "def collate_fn(batch):\n",
        "    inputs, labels = zip(*batch)\n",
        "    inputs = [torch.tensor(x) for x in inputs]\n",
        "    padded_inputs = pad_sequence(inputs, batch_first=True, padding_value=0)  # Pad sequences\n",
        "    labels = torch.stack(labels)\n",
        "    return padded_inputs, labels\n",
        "\n",
        "def collate_fn_cnn(batch, max_length=100):\n",
        "    inputs, labels = zip(*batch)\n",
        "    inputs = [x[:max_length] if len(x) >= max_length else torch.cat([x, torch.zeros(max_length - len(x), dtype=torch.long)]) for x in inputs]\n",
        "    inputs = torch.stack(inputs)\n",
        "    labels = torch.tensor(labels)\n",
        "    return inputs, labels\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "uEY6zYYKW6t1"
      },
      "outputs": [],
      "source": [
        "# SentimentRNN class\n",
        "class SentimentRNN(nn.Module):\n",
        "    def __init__(self, embedding_matrix, hidden_size, output_size,\n",
        "                 rnn_type=\"RNN\", num_layers=1, use_bidirectional=False,\n",
        "                 use_dropout=False, use_batch_norm=False, use_layer_norm=False,\n",
        "                 aggregation_method='last_hidden', freeze_embeddings=True):\n",
        "        super(SentimentRNN, self).__init__()\n",
        "\n",
        "        vocab_size, embedding_dim = embedding_matrix.shape\n",
        "\n",
        "        # Embedding layer using pre-trained embeddings\n",
        "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
        "        self.embedding.weight = nn.Parameter(torch.tensor(embedding_matrix, dtype=torch.float32))\n",
        "        self.embedding.weight.requires_grad = not freeze_embeddings  # Control freezing\n",
        "\n",
        "        # Choose RNN type dynamically\n",
        "        if rnn_type == \"LSTM\":\n",
        "            self.rnn = nn.LSTM(embedding_dim, hidden_size, num_layers=num_layers,\n",
        "                               batch_first=True, bidirectional=use_bidirectional)\n",
        "        elif rnn_type == \"GRU\":\n",
        "            self.rnn = nn.GRU(embedding_dim, hidden_size, num_layers=num_layers,\n",
        "                              batch_first=True, bidirectional=use_bidirectional)\n",
        "        else:  # Default to Simple RNN\n",
        "            self.rnn = nn.RNN(embedding_dim, hidden_size, num_layers=num_layers,\n",
        "                              batch_first=True, bidirectional=use_bidirectional)\n",
        "\n",
        "        # Store the aggregation method\n",
        "        self.aggregation_method = aggregation_method\n",
        "\n",
        "        # Determine the final hidden size after aggregation\n",
        "        if aggregation_method == 'last_hidden':\n",
        "            if use_bidirectional:\n",
        "                final_hidden_size = hidden_size * 2\n",
        "            else:\n",
        "                final_hidden_size = hidden_size\n",
        "        else:\n",
        "            if use_bidirectional:\n",
        "                final_hidden_size = hidden_size * 2\n",
        "            else:\n",
        "                final_hidden_size = hidden_size\n",
        "\n",
        "        # Fully connected layer for classification\n",
        "        self.fc = nn.Linear(final_hidden_size, output_size)\n",
        "\n",
        "        # Optional Regularization Layers\n",
        "        self.use_dropout = use_dropout\n",
        "        self.use_batch_norm = use_batch_norm\n",
        "        self.use_layer_norm = use_layer_norm\n",
        "\n",
        "        if self.use_dropout:\n",
        "            self.dropout = nn.Dropout(0.3)  # Dropout rate of 0.3\n",
        "\n",
        "        if self.use_batch_norm:\n",
        "            self.batch_norm = nn.BatchNorm1d(final_hidden_size)\n",
        "\n",
        "        if self.use_layer_norm:\n",
        "            self.layer_norm = nn.LayerNorm(final_hidden_size)\n",
        "\n",
        "    def forward(self, x):\n",
        "        embedded = self.embedding(x)\n",
        "        output, hidden = self.rnn(embedded)\n",
        "\n",
        "        # For LSTM, hidden is a tuple of (h_n, c_n); use h_n\n",
        "        if isinstance(hidden, tuple):\n",
        "            hidden = hidden[0]  # h_n\n",
        "\n",
        "        if self.aggregation_method == 'last_hidden':\n",
        "            if self.rnn.bidirectional:\n",
        "                final_output = torch.cat((hidden[-2,:,:], hidden[-1,:,:]), dim=1)\n",
        "            else:\n",
        "                final_output = hidden[-1,:,:]\n",
        "        elif self.aggregation_method == 'mean_pooling':\n",
        "            final_output = output.mean(dim=1)\n",
        "        elif self.aggregation_method == 'max_pooling':\n",
        "            final_output, _ = torch.max(output, dim=1)\n",
        "        else:\n",
        "            raise ValueError(f\"Unknown aggregation method: {self.aggregation_method}\")\n",
        "\n",
        "        # Apply optional regularization layers\n",
        "        if self.use_batch_norm:\n",
        "            final_output = self.batch_norm(final_output)\n",
        "\n",
        "        if self.use_layer_norm:\n",
        "            final_output = self.layer_norm(final_output)\n",
        "\n",
        "        if self.use_dropout:\n",
        "            final_output = self.dropout(final_output)\n",
        "\n",
        "        return self.fc(final_output)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "enikXnhwW6rA"
      },
      "outputs": [],
      "source": [
        "# SentimentCNN class\n",
        "class SentimentCNN(nn.Module):\n",
        "    def __init__(self, embedding_matrix, output_size, freeze_embeddings=True,\n",
        "                 num_filters=100, filter_sizes=[3,4,5], dropout_rate=0.5):\n",
        "        super(SentimentCNN, self).__init__()\n",
        "\n",
        "        vocab_size, embedding_dim = embedding_matrix.shape\n",
        "\n",
        "        # Embedding layer using pre-trained embeddings\n",
        "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
        "        self.embedding.weight = nn.Parameter(torch.tensor(embedding_matrix, dtype=torch.float32))\n",
        "        self.embedding.weight.requires_grad = not freeze_embeddings  # Control freezing\n",
        "\n",
        "        # Convolutional layers with multiple filter sizes\n",
        "        self.convs = nn.ModuleList([\n",
        "            nn.Conv2d(in_channels=1, out_channels=num_filters, kernel_size=(fs, embedding_dim))\n",
        "            for fs in filter_sizes\n",
        "        ])\n",
        "\n",
        "        # Fully connected layer\n",
        "        self.fc = nn.Linear(len(filter_sizes) * num_filters, output_size)\n",
        "\n",
        "        # Dropout layer\n",
        "        self.dropout = nn.Dropout(dropout_rate)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.embedding(x)  # Shape: (batch_size, seq_length, embedding_dim)\n",
        "        x = x.unsqueeze(1)  # Add channel dimension: (batch_size, 1, seq_length, embedding_dim)\n",
        "\n",
        "        # Apply convolution and ReLU activation\n",
        "        conv_outs = [torch.relu(conv(x)).squeeze(3) for conv in self.convs]\n",
        "\n",
        "        # Apply max pooling over the sequence length\n",
        "        pooled_outs = [torch.max(conv_out, dim=2)[0] for conv_out in conv_outs]\n",
        "\n",
        "        # Concatenate pooled outputs\n",
        "        cat = torch.cat(pooled_outs, dim=1)\n",
        "\n",
        "        # Apply dropout\n",
        "        out = self.dropout(cat)\n",
        "\n",
        "        # Fully connected layer\n",
        "        out = self.fc(out)\n",
        "        return out\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "zjircnwJW6oR"
      },
      "outputs": [],
      "source": [
        "# Function to get optimizer\n",
        "def get_optimizer(params, model):\n",
        "    optimizer_type = params[\"optimizer_type\"]\n",
        "    lr = params[\"learning_rate\"]\n",
        "    weight_decay = params.get(\"weight_decay\", 0)  # Default to 0 if not specified\n",
        "\n",
        "    if optimizer_type == \"SGD\":\n",
        "        momentum = params.get(\"momentum\", 0)  # Default to 0 if not specified\n",
        "        return optim.SGD(model.parameters(), lr=lr, momentum=momentum, weight_decay=weight_decay)\n",
        "\n",
        "    elif optimizer_type == \"Adam\":\n",
        "        return optim.Adam(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
        "\n",
        "    else:\n",
        "        raise ValueError(f\"Unknown optimizer type: {optimizer_type}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "7jvCWISIW6im"
      },
      "outputs": [],
      "source": [
        "# Function to train the model\n",
        "def train_model(model, train_loader, valid_loader, test_loader, optimizer, epochs, patience, scheduler_step_size, scheduler_gamma, device):\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=scheduler_step_size, gamma=scheduler_gamma)\n",
        "\n",
        "    train_losses = []\n",
        "    val_accuracies = []\n",
        "    test_accuracies = []\n",
        "    best_val_accuracy = 0\n",
        "    epochs_no_improve = 0\n",
        "\n",
        "    for epoch in range(1, epochs+1):\n",
        "        model.train()\n",
        "        running_loss = 0.0\n",
        "        for inputs, labels in train_loader:\n",
        "            inputs, labels = inputs.to(device), labels.to(device)\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(inputs)\n",
        "            loss = criterion(outputs, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            running_loss += loss.item()\n",
        "\n",
        "        avg_train_loss = running_loss / len(train_loader)\n",
        "        train_losses.append(avg_train_loss)\n",
        "\n",
        "        # Validation\n",
        "        val_accuracy = evaluate_accuracy(model, valid_loader, device)\n",
        "        val_accuracies.append(val_accuracy)\n",
        "\n",
        "        # Record metrics every 10 epochs\n",
        "        if epoch % 10 == 0:\n",
        "            print(f'Epoch {epoch}, Loss: {avg_train_loss:.4f}, Validation Accuracy: {val_accuracy:.2f}%')\n",
        "\n",
        "        # Early stopping logic\n",
        "        if val_accuracy > best_val_accuracy:\n",
        "            best_val_accuracy = val_accuracy\n",
        "            epochs_no_improve = 0\n",
        "            # Save the best model\n",
        "            torch.save(model.state_dict(), f'best_model.pt')\n",
        "            print(\"best_epoch: \", epoch+1)\n",
        "        else:\n",
        "            epochs_no_improve += 1\n",
        "\n",
        "        if epochs_no_improve >= patience:\n",
        "            print(f\"Early stopping at epoch {epoch + 1}\")\n",
        "            break\n",
        "\n",
        "        scheduler.step()\n",
        "\n",
        "    # Load the best model before evaluating on test set\n",
        "    # best_epoch = val_accuracies.index(best_val_accuracy) + 1\n",
        "    model.load_state_dict(torch.load(f'best_model.pt'))\n",
        "\n",
        "    # Test Accuracy\n",
        "    test_accuracy = evaluate_accuracy(model, test_loader, device)\n",
        "    test_accuracies = [test_accuracy] * len(train_losses)\n",
        "    print(f'Test Accuracy: {test_accuracy:.2f}%')\n",
        "\n",
        "    return train_losses, val_accuracies, test_accuracies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "4dvJWIl8W6Z5"
      },
      "outputs": [],
      "source": [
        "# Function to evaluate accuracy\n",
        "def evaluate_accuracy(model, data_loader, device):\n",
        "    model.eval()\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    with torch.no_grad():\n",
        "        for inputs, labels in data_loader:\n",
        "            inputs, labels = inputs.to(device), labels.to(device)\n",
        "            outputs = model(inputs)\n",
        "            _, predicted = outputs.max(1)\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "    accuracy = 100 * correct / total\n",
        "    return accuracy\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "t4_SryycXG54"
      },
      "outputs": [],
      "source": [
        "# # Function to run experiments from CSV\n",
        "# def run_experiments_from_csv(params_csv, train_dataset, validation_dataset, test_dataset):\n",
        "#     csv_filename = 'training_results.csv'\n",
        "#     fieldnames = ['Epoch', 'Train Loss', 'Validation Accuracy', 'Test Accuracy',\n",
        "#                   'model_type', 'rnn_type', 'num_layers', 'bidirectional', 'dropout', 'batch_norm', 'layer_norm',\n",
        "#                   'aggregation_method', 'optimizer_type', 'learning_rate', 'momentum', 'weight_decay',\n",
        "#                   'batch_size', 'epochs', 'patience', 'hidden_size', 'output_size',\n",
        "#                   'freeze_embeddings', 'num_filters', 'filter_sizes', 'dropout_rate', 'oov_handling_method',\n",
        "#                   'embedding_dim', 'glove_file_path']\n",
        "\n",
        "#     # Open the CSV file with the configurations\n",
        "#     with open(params_csv, mode='r') as params_file:\n",
        "#         reader = csv.DictReader(params_file)\n",
        "\n",
        "#         # Prepare the results CSV to store the results for each run\n",
        "#         with open(csv_filename, mode='w', newline='') as result_file:\n",
        "#             writer = csv.DictWriter(result_file, fieldnames=fieldnames)\n",
        "#             writer.writeheader()\n",
        "\n",
        "#             for config in reader:\n",
        "#                 # Convert necessary parameters to appropriate types\n",
        "#                 config['num_layers'] = int(config['num_layers']) if config['num_layers'] != 'N/A' else None\n",
        "#                 config['use_bidirectional'] = config['use_bidirectional'] == 'True' if config['use_bidirectional'] != 'N/A' else None\n",
        "#                 config['use_dropout'] = config['use_dropout'] == 'True'\n",
        "#                 config['use_batch_norm'] = config['use_batch_norm'] == 'True'\n",
        "#                 config['use_layer_norm'] = config['use_layer_norm'] == 'True'\n",
        "#                 config['batch_size'] = int(config['batch_size'])\n",
        "#                 config['epochs'] = int(config['epochs'])\n",
        "#                 config['patience'] = int(config['patience'])\n",
        "#                 config['hidden_size'] = int(config['hidden_size']) if config['hidden_size'] != 'N/A' else None\n",
        "#                 config['output_size'] = int(config['output_size'])\n",
        "#                 config['learning_rate'] = float(config['learning_rate'])\n",
        "#                 config['momentum'] = float(config['momentum'])\n",
        "#                 config['weight_decay'] = float(config['weight_decay'])\n",
        "#                 config['freeze_embeddings'] = config['freeze_embeddings'] == 'True'\n",
        "#                 config['num_filters'] = int(config['num_filters']) if config['num_filters'] != 'N/A' else None\n",
        "#                 config['filter_sizes'] = [int(fs) for fs in config['filter_sizes'].split(',')] if config['filter_sizes'] != 'N/A' else None\n",
        "#                 config['dropout_rate'] = float(config['dropout_rate']) if config['dropout_rate'] != 'N/A' else None\n",
        "#                 config['aggregation_method'] = config['aggregation_method']\n",
        "#                 config['oov_handling_method'] = config['oov_handling_method']\n",
        "#                 config['embedding_dim'] = int(config['embedding_dim'])\n",
        "#                 config['glove_file_path'] = config['glove_file_path']\n",
        "\n",
        "#                 # Load GloVe embeddings\n",
        "#                 glove_embeddings = load_glove_embeddings(config['glove_file_path'])\n",
        "\n",
        "#                 # Build vocabulary and create embedding matrix\n",
        "#                 vocabulary = build_vocabulary(train_dataset, config['oov_handling_method'])\n",
        "#                 embedding_matrix = create_embedding_matrix(config['embedding_dim'], vocabulary, glove_embeddings, config['oov_handling_method'])\n",
        "\n",
        "#                 # Build word_to_index mapping\n",
        "#                 word_to_index = {word: idx for idx, word in enumerate(vocabulary)}\n",
        "\n",
        "#                 # Initialize the model based on 'model_type'\n",
        "#                 if config['model_type'] == 'RNN':\n",
        "#                     model = SentimentRNN(\n",
        "#                         embedding_matrix=embedding_matrix,\n",
        "#                         hidden_size=config['hidden_size'],\n",
        "#                         output_size=config['output_size'],\n",
        "#                         rnn_type=config['rnn_type'],\n",
        "#                         num_layers=config['num_layers'],\n",
        "#                         use_bidirectional=config['use_bidirectional'],\n",
        "#                         use_dropout=config['use_dropout'],\n",
        "#                         use_batch_norm=config['use_batch_norm'],\n",
        "#                         use_layer_norm=config['use_layer_norm'],\n",
        "#                         aggregation_method=config['aggregation_method'],\n",
        "#                         freeze_embeddings=config['freeze_embeddings']\n",
        "#                     )\n",
        "#                     collate_function = collate_fn\n",
        "#                 elif config['model_type'] == 'CNN':\n",
        "#                     model = SentimentCNN(\n",
        "#                         embedding_matrix=embedding_matrix,\n",
        "#                         output_size=config['output_size'],\n",
        "#                         freeze_embeddings=config['freeze_embeddings'],\n",
        "#                         num_filters=config['num_filters'],\n",
        "#                         filter_sizes=config['filter_sizes'],\n",
        "#                         dropout_rate=config['dropout_rate']\n",
        "#                     )\n",
        "#                     collate_function = collate_fn_cnn\n",
        "#                 else:\n",
        "#                     raise ValueError(f\"Unknown model type: {config['model_type']}\")\n",
        "\n",
        "#                 # Get the optimizer dynamically based on the config\n",
        "#                 optimizer_params = {\n",
        "#                     'optimizer_type': config['optimizer_type'],\n",
        "#                     'learning_rate': config['learning_rate'],\n",
        "#                     'momentum': config['momentum'],\n",
        "#                     'weight_decay': config['weight_decay']\n",
        "#                 }\n",
        "#                 optimizer = get_optimizer(optimizer_params, model)\n",
        "\n",
        "#                 # Create DataLoaders using the appropriate collate function\n",
        "#                 train_loader = DataLoader(TextDataset(train_dataset, vocabulary, word_to_index, config['oov_handling_method']), batch_size=config['batch_size'], shuffle=True, collate_fn=collate_function)\n",
        "#                 valid_loader = DataLoader(TextDataset(validation_dataset, vocabulary, word_to_index, config['oov_handling_method']), batch_size=config['batch_size'], collate_fn=collate_function)\n",
        "#                 test_loader = DataLoader(TextDataset(test_dataset, vocabulary, word_to_index, config['oov_handling_method']), batch_size=config['batch_size'], collate_fn=collate_function)\n",
        "\n",
        "#                 device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "#                 model.to(device)\n",
        "\n",
        "#                 # Train the model\n",
        "#                 train_loss, val_accuracy, test_accuracy = train_model(\n",
        "#                     model=model,\n",
        "#                     train_loader=train_loader,\n",
        "#                     valid_loader=valid_loader,\n",
        "#                     test_loader=test_loader,\n",
        "#                     optimizer=optimizer,\n",
        "#                     epochs=config['epochs'],\n",
        "#                     patience=config['patience'],\n",
        "#                     scheduler_step_size=3,\n",
        "#                     scheduler_gamma=0.1,\n",
        "#                     device=device\n",
        "#                 )\n",
        "\n",
        "#                 # Log results to CSV after training completes\n",
        "#                 for epoch in range(len(train_loss)):  # Iterate through each epoch's results\n",
        "#                     writer.writerow({\n",
        "#                         'Epoch': epoch + 1,\n",
        "#                         'Train Loss': train_loss[epoch],\n",
        "#                         'Validation Accuracy': val_accuracy[epoch],\n",
        "#                         'Test Accuracy': test_accuracy[epoch],\n",
        "#                         'model_type': config['model_type'],\n",
        "#                         'rnn_type': config['rnn_type'],\n",
        "#                         'num_layers': config['num_layers'],\n",
        "#                         'bidirectional': config['use_bidirectional'],\n",
        "#                         'dropout': config['use_dropout'],\n",
        "#                         'batch_norm': config['use_batch_norm'],\n",
        "#                         'layer_norm': config['use_layer_norm'],\n",
        "#                         'aggregation_method': config['aggregation_method'],\n",
        "#                         'optimizer_type': config['optimizer_type'],\n",
        "#                         'learning_rate': config['learning_rate'],\n",
        "#                         'momentum': config['momentum'],\n",
        "#                         'weight_decay': config['weight_decay'],\n",
        "#                         'batch_size': config['batch_size'],\n",
        "#                         'epochs': config['epochs'],\n",
        "#                         'patience': config['patience'],\n",
        "#                         'hidden_size': config['hidden_size'],\n",
        "#                         'output_size': config['output_size'],\n",
        "#                         'freeze_embeddings': config['freeze_embeddings'],\n",
        "#                         'num_filters': config['num_filters'],\n",
        "#                         'filter_sizes': config['filter_sizes'],\n",
        "#                         'dropout_rate': config['dropout_rate'],\n",
        "#                         'oov_handling_method': config['oov_handling_method'],\n",
        "#                         'embedding_dim': config['embedding_dim'],\n",
        "#                         'glove_file_path': config['glove_file_path']\n",
        "#                     })\n",
        "\n",
        "#     print(f\"Experiments completed. Results saved to {csv_filename}.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G6apxqGT4xMR",
        "outputId": "e97b208f-4f02-46c5-86bc-62f53723173c"
      },
      "outputs": [],
      "source": [
        "# # To download experiment params from gdrive (not used now, use the create csv below for colab env)\n",
        "# experiment_param_file_id = '1NsO-vQyC_CJjykdd3gbea1CdzHT5pKyY'\n",
        "# experiment_param_file = 'experiment_params.csv'\n",
        "# !gdown {experiment_param_file_id} -O {experiment_param_file}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "g651a7qch7vn"
      },
      "outputs": [],
      "source": [
        "# # Function to create CSV in colab environment\n",
        "\n",
        "# import csv\n",
        "\n",
        "# # String data in CSV format\n",
        "# header = [\n",
        "#     \"model_type\",          # Possible Values: 'RNN', 'CNN'                                  | Meaning: Specifies the type of model to use.                    | Type: str\n",
        "#     \"rnn_type\",            # Possible Values: 'RNN', 'LSTM', 'GRU'                          | Meaning: The type of recurrent layer to use in the RNN.         | Type: str\n",
        "#     \"num_layers\",          # Possible Values: Positive integers (e.g., 1, 2)                | Meaning: The number of layers in the RNN.                       | Type: int\n",
        "#     \"use_bidirectional\",   # Possible Values: True, False                                   | Meaning: Whether to use a bidirectional RNN.                    | Type: bool\n",
        "#     \"use_dropout\",         # Possible Values: True, False                                   | Meaning: Whether to apply dropout regularization in the model.  | Type: bool\n",
        "#     \"use_batch_norm\",      # Possible Values: True, False                                   | Meaning: Whether to apply batch normalization.                  | Type: bool\n",
        "#     \"use_layer_norm\",      # Possible Values: True, False                                   | Meaning: Whether to apply layer normalization.                  | Type: bool\n",
        "#     \"aggregation_method\",  # Possible Values: 'last_hidden', 'mean_pooling', 'max_pooling'  | Meaning: Method to aggregate the sequence of hidden states.     | Type: str\n",
        "#     \"optimizer_type\",      # Possible Values: 'SGD', 'Adam'                                 | Meaning: The optimizer to use.                                  | Type: str\n",
        "#     \"learning_rate\",       # Possible Values: Positive floats (e.g., 0.01)                  | Meaning: Learning rate for the optimizer.                       | Type: float\n",
        "#     \"momentum\",            # Possible Values: Floats between 0 and 1 (e.g., 0.9)            | Meaning: Momentum factor (only for 'SGD').                      | Type: float\n",
        "#     \"weight_decay\",        # Possible Values: Non-negative floats (e.g., 0.0001)            | Meaning: Weight decay (L2 regularization).                      | Type: float\n",
        "#     \"batch_size\",          # Possible Values: Positive integers (e.g., 32)                  | Meaning: Batch size during training.                            | Type: int\n",
        "#     \"epochs\",              # Possible Values: Positive integers (e.g., 20)                  | Meaning: Maximum number of training epochs.                     | Type: int\n",
        "#     \"patience\",            # Possible Values: Positive integers (e.g., 3)                   | Meaning: Number of epochs with no improvement before stopping.  | Type: int\n",
        "#     \"hidden_size\",         # Possible Values: Positive integers (e.g., 128)                 | Meaning: Number of features in the hidden state.                | Type: int\n",
        "#     \"output_size\",         # Possible Values: Positive integers (e.g., 2)                   | Meaning: Number of output classes.                              | Type: int\n",
        "#     \"freeze_embeddings\",   # Possible Values: True, False                                   | Meaning: Whether to freeze the embedding layer weights.         | Type: bool\n",
        "#     \"num_filters\",         # Possible Values: Positive integers (e.g., 100)                 | Meaning: Number of filters in the CNN.                          | Type: int\n",
        "#     \"filter_sizes\",        # Possible Values: List of positive integers (e.g., [3,4,5])     | Meaning: Sizes of convolutional filters.                        | Type: list of int\n",
        "#     \"dropout_rate\",        # Possible Values: Floats between 0 and 1 (e.g., 0.5)            | Meaning: Dropout rate after convolutional layers.               | Type: float\n",
        "#     \"oov_handling_method\", # Possible Values: 'unknown_token', 'random', 'none'             | Meaning: Strategy to handle OOV words.                          | Type: str\n",
        "#     \"embedding_dim\",       # Possible Values: Positive integers (e.g., 50, 100, 200, 300)   | Meaning: Dimension of word embeddings.                          | Type: int\n",
        "#     \"glove_file_path\"      # Possible Values: String (file path)                            | Meaning: Path to the GloVe embedding file.                      | Type: str\n",
        "# ]\n",
        "\n",
        "\n",
        "# # data = [\n",
        "# #     [\n",
        "# #         \"RNN\", \"RNN\", 1, False, False, False, False, \"last_hidden\", \"Adam\", 0.01, 0, 0.0001, 32, 100, 3, 128, 2, True, 0, 0, 0, \"none\", 100, \"glove.6B.100d.txt\"\n",
        "# #     ]\n",
        "# # ]\n",
        "\n",
        "# # Define the filename\n",
        "# filename = \"experiment_params.csv\"\n",
        "\n",
        "# # Write the data to a CSV file\n",
        "# with open(filename, mode='w', newline='') as file:\n",
        "#     writer = csv.writer(file)\n",
        "#     writer.writerow(header)\n",
        "#     writer.writerows(data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.model_selection import ParameterGrid\n",
        "\n",
        "part_2_rnn_param_grid = { # rnn, fixed embeddings no OOV soln\n",
        "    \"optimizer_type\": [\"Adam\", \"SGD\"],\n",
        "    \"learning_rate\": [0.01, 0.001],\n",
        "    \"momentum\": [0, 0.9],  # Used only for SGD\n",
        "    \"weight_decay\": [0.0001, 0.001],\n",
        "    \"batch_size\": [32, 64, 128],\n",
        "    \"epochs\": [100, 200],\n",
        "    \"patience\": [10],\n",
        "\n",
        "    \"model_type\": [\"RNN\"],\n",
        "    \"rnn_type\": [\"RNN\"],\n",
        "\n",
        "    \"num_layers\": [1, 2, 3],\n",
        "    \"use_bidirectional\": [False],\n",
        "    \"use_dropout\": [True, False],\n",
        "    \"use_batch_norm\": [True, False],\n",
        "    \"use_layer_norm\": [True, False],\n",
        "    \"aggregation_method\": [\"last_hidden\"],\n",
        "    \n",
        "    \"hidden_size\": [64, 128, 256],\n",
        "    \"output_size\": [2],\n",
        "    \"freeze_embeddings\": [True], # keep embeddings fixed\n",
        "\n",
        "    \"oov_handling_method\": [\"none\"], # no OOV soln\n",
        "    \"embedding_dim\": [100], # default\n",
        "    \"glove_file_path\": [\"glove.6B.100d.txt\"] # default\n",
        "}\n",
        "\n",
        "part_3_rnn_param_grid = { # rnn, update embeddings with OOV soln\n",
        "    \"optimizer_type\": [\"Adam\", \"SGD\"],\n",
        "    \"learning_rate\": [0.01, 0.001],\n",
        "    \"momentum\": [0.9, 0.95],  # Used only for SGD\n",
        "    \"weight_decay\": [0.0001, 0.001],\n",
        "    \"batch_size\": [32, 64, 128],\n",
        "    \"epochs\": [100, 200],\n",
        "    \"patience\": [10],\n",
        "\n",
        "    \"model_type\": [\"RNN\"],\n",
        "    \"rnn_type\": [\"RNN\"],\n",
        "\n",
        "    \"num_layers\": [1, 2, 3],\n",
        "    \"use_bidirectional\": [False],\n",
        "    \"use_dropout\": [True],\n",
        "    \"use_batch_norm\": [True, False],\n",
        "    \"use_layer_norm\": [True, False],\n",
        "    \"aggregation_method\": [\"last_hidden\"],\n",
        "    \n",
        "    \"hidden_size\": [64, 128, 256],\n",
        "    \"output_size\": [2],\n",
        "    \"freeze_embeddings\": [False], # update embeddings during training\n",
        "\n",
        "    \"oov_handling_method\": [\"unknown_token\", \"random\"], # apply soln of OOV and train\n",
        "    \"embedding_dim\": [100], # default\n",
        "    \"glove_file_path\": [\"glove.6B.100d.txt\"] # default\n",
        "}\n",
        "\n",
        "part_3_blstm_bgru_param_grid = { # replace rnn with blstm/bgru, update embeddings with OOV soln\n",
        "    \"optimizer_type\": [\"Adam\", \"SGD\"],\n",
        "    \"learning_rate\": [0.01, 0.001],\n",
        "    \"momentum\": [0.9, 0.95],  # Used only for SGD\n",
        "    \"weight_decay\": [0.0001, 0.001],\n",
        "    \"batch_size\": [32, 64, 128],\n",
        "    \"epochs\": [100,200],\n",
        "    \"patience\": [10],\n",
        "\n",
        "    \"model_type\": [\"RNN\"],\n",
        "    \"rnn_type\": [\"LSTM\", \"GRU\"],\n",
        "\n",
        "    \"num_layers\": [1, 2, 3],\n",
        "    \"use_bidirectional\": [True],\n",
        "    \"use_dropout\": [True],\n",
        "    \"use_batch_norm\": [True, False],\n",
        "    \"use_layer_norm\": [True, False],\n",
        "    \"aggregation_method\": [\"last_hidden\", \"mean_pooling\", \"max_pooling\"],\n",
        "    \n",
        "    \"hidden_size\": [64, 128, 256],\n",
        "    \"output_size\": [2],\n",
        "    \"freeze_embeddings\": [False], # update embeddings during training\n",
        "\n",
        "    \"oov_handling_method\": [\"unknown_token\", \"random\"], # apply soln of OOV and train\n",
        "    \"embedding_dim\": [100], # default\n",
        "    \"glove_file_path\": [\"glove.6B.100d.txt\"] # default\n",
        "}\n",
        "\n",
        "part_3_cnn_param_grid = { # replace rnn with cnn, update embeddings with OOV soln\n",
        "    \"optimizer_type\": [\"Adam\", \"SGD\"],\n",
        "    \"learning_rate\": [0.01, 0.001],\n",
        "    \"momentum\": [0.9, 0.95],  # Used only for SGD\n",
        "    \"weight_decay\": [0.0001, 0.001],\n",
        "    \"batch_size\": [32, 64, 128],\n",
        "    \"epochs\": [100, 200],\n",
        "    \"patience\": [10],\n",
        "\n",
        "    \"model_type\": [\"CNN\"],\n",
        "    \n",
        "    \"num_filters\": [100, 128, 256],  # CNN only\n",
        "    \"filter_sizes\": [[3, 4, 5], [2, 3]],\n",
        "    \"dropout_rate\": [0.3, 0.5, 0.7],  # CNN only\n",
        "    \"output_size\": [2],\n",
        "    \"freeze_embeddings\": [False], # update embeddings during training\n",
        "\n",
        "    \"oov_handling_method\": [\"unknown_token\", \"random\"], # apply soln of OOV and train\n",
        "    \"embedding_dim\": [100], # default\n",
        "    \"glove_file_path\": [\"glove.6B.100d.txt\"] # default\n",
        "}\n",
        "\n",
        "# Quest"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {},
      "outputs": [],
      "source": [
        "import itertools\n",
        "import csv"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Function to run experiments from CSV\n",
        "def run_part2_rnn_experiments_from_paramgrid(train_dataset, validation_dataset, test_dataset):\n",
        "    param_combinations = list(itertools.product(*part_2_rnn_param_grid.values())) # Initialize a list to hold results\n",
        "    results = [] # Initialize a list to hold results\n",
        "    \n",
        "    # Iterate through each combination of parameters\n",
        "    for param in param_combinations:\n",
        "        config = dict(zip(part_2_rnn_param_grid.keys(), param))\n",
        "        print(config)\n",
        "\n",
        "        # Load GloVe embeddings\n",
        "        glove_embeddings = load_glove_embeddings(config['glove_file_path'])\n",
        "\n",
        "        # Build vocabulary and create embedding matrix\n",
        "        vocabulary = build_vocabulary(train_dataset, config['oov_handling_method'])\n",
        "        embedding_matrix = create_embedding_matrix(config['embedding_dim'], vocabulary, glove_embeddings, config['oov_handling_method'])\n",
        "\n",
        "        # Build word_to_index mapping\n",
        "        word_to_index = {word: idx for idx, word in enumerate(vocabulary)}\n",
        "\n",
        "        # Initialize the model based on 'model_type'\n",
        "        if config['model_type'] == 'RNN':\n",
        "            model = SentimentRNN(\n",
        "                embedding_matrix=embedding_matrix,\n",
        "                hidden_size=config['hidden_size'],\n",
        "                output_size=config['output_size'],\n",
        "                rnn_type=config['rnn_type'],\n",
        "                num_layers=config['num_layers'],\n",
        "                use_bidirectional=config['use_bidirectional'],\n",
        "                use_dropout=config['use_dropout'],\n",
        "                use_batch_norm=config['use_batch_norm'],\n",
        "                use_layer_norm=config['use_layer_norm'],\n",
        "                aggregation_method=config['aggregation_method'],\n",
        "                freeze_embeddings=config['freeze_embeddings']\n",
        "            )\n",
        "            collate_function = collate_fn\n",
        "        else:\n",
        "            raise ValueError(f\"Unknown model type: {config['model_type']}\")\n",
        "\n",
        "        # Get the optimizer dynamically based on the config\n",
        "        optimizer_params = {\n",
        "            'optimizer_type': config['optimizer_type'],\n",
        "            'learning_rate': config['learning_rate'],\n",
        "            'momentum': config['momentum'],\n",
        "            'weight_decay': config['weight_decay']\n",
        "        }\n",
        "        optimizer = get_optimizer(optimizer_params, model)\n",
        "\n",
        "        # Create DataLoaders using the appropriate collate function\n",
        "        train_loader = DataLoader(TextDataset(train_dataset, vocabulary, word_to_index, config['oov_handling_method']), batch_size=config['batch_size'], shuffle=True, collate_fn=collate_function)\n",
        "        valid_loader = DataLoader(TextDataset(validation_dataset, vocabulary, word_to_index, config['oov_handling_method']), batch_size=config['batch_size'], collate_fn=collate_function)\n",
        "        test_loader = DataLoader(TextDataset(test_dataset, vocabulary, word_to_index, config['oov_handling_method']), batch_size=config['batch_size'], collate_fn=collate_function)\n",
        "\n",
        "        device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "        model.to(device)\n",
        "        \n",
        "        # Train the model\n",
        "        train_loss, val_accuracy, test_accuracy = train_model(\n",
        "            model=model,\n",
        "            train_loader=train_loader,\n",
        "            valid_loader=valid_loader,\n",
        "            test_loader=test_loader,\n",
        "            optimizer=optimizer,\n",
        "            epochs=config['epochs'],\n",
        "            patience=config['patience'],\n",
        "            scheduler_step_size=3,\n",
        "            scheduler_gamma=0.1,\n",
        "            device=device\n",
        "        )\n",
        "        \n",
        "        for epoch in range(len(train_loss)):  # Iterate through each epoch's results\n",
        "            config['Epoch'] = epoch\n",
        "            config['Train Loss'] = train_loss\n",
        "            config['Validation Accuracy'] = val_accuracy\n",
        "            config['Test Accuracy'] = test_accuracy\n",
        "\n",
        "            results.append(config)  # Append the config to results list\n",
        "\n",
        "    # Define CSV header\n",
        "    header = list(param_grid.keys()) + ['Epoch', 'Train Loss', 'Validation Accuracy', 'Test Accuracy']\n",
        "\n",
        "    # Write results to a CSV file\n",
        "    with open('part2_rnn_results.csv', mode='w', newline='') as csvfile:\n",
        "        writer = csv.DictWriter(csvfile, fieldnames=header)\n",
        "        writer.writeheader()\n",
        "        writer.writerows(results)\n",
        "\n",
        "    print(\"Parameter combinations and performances recorded in results.csv\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Function to run experiments from CSV\n",
        "def run_part3_rnn_experiments_from_paramgrid(train_dataset, validation_dataset, test_dataset):\n",
        "    param_combinations = list(itertools.product(*part_3_rnn_param_grid.values())) # Initialize a list to hold results\n",
        "    results = [] # Initialize a list to hold results\n",
        "    \n",
        "    # Iterate through each combination of parameters\n",
        "    for param in param_combinations:\n",
        "        config = dict(zip(part_3_rnn_param_grid.keys(), param))\n",
        "\n",
        "        # Load GloVe embeddings\n",
        "        glove_embeddings = load_glove_embeddings(config['glove_file_path'])\n",
        "\n",
        "        # Build vocabulary and create embedding matrix\n",
        "        vocabulary = build_vocabulary(train_dataset, config['oov_handling_method'])\n",
        "        embedding_matrix = create_embedding_matrix(config['embedding_dim'], vocabulary, glove_embeddings, config['oov_handling_method'])\n",
        "\n",
        "        # Build word_to_index mapping\n",
        "        word_to_index = {word: idx for idx, word in enumerate(vocabulary)}\n",
        "\n",
        "        # Initialize the model based on 'model_type'\n",
        "        if config['model_type'] == 'RNN':\n",
        "            model = SentimentRNN(\n",
        "                embedding_matrix=embedding_matrix,\n",
        "                hidden_size=config['hidden_size'],\n",
        "                output_size=config['output_size'],\n",
        "                rnn_type=config['rnn_type'],\n",
        "                num_layers=config['num_layers'],\n",
        "                use_bidirectional=config['use_bidirectional'],\n",
        "                use_dropout=config['use_dropout'],\n",
        "                use_batch_norm=config['use_batch_norm'],\n",
        "                use_layer_norm=config['use_layer_norm'],\n",
        "                aggregation_method=config['aggregation_method'],\n",
        "                freeze_embeddings=config['freeze_embeddings']\n",
        "            )\n",
        "            collate_function = collate_fn\n",
        "        else:\n",
        "            raise ValueError(f\"Unknown model type: {config['model_type']}\")\n",
        "\n",
        "        # Get the optimizer dynamically based on the config\n",
        "        optimizer_params = {\n",
        "            'optimizer_type': config['optimizer_type'],\n",
        "            'learning_rate': config['learning_rate'],\n",
        "            'momentum': config['momentum'],\n",
        "            'weight_decay': config['weight_decay']\n",
        "        }\n",
        "        optimizer = get_optimizer(optimizer_params, model)\n",
        "\n",
        "        # Create DataLoaders using the appropriate collate function\n",
        "        train_loader = DataLoader(TextDataset(train_dataset, vocabulary, word_to_index, config['oov_handling_method']), batch_size=config['batch_size'], shuffle=True, collate_fn=collate_function)\n",
        "        valid_loader = DataLoader(TextDataset(validation_dataset, vocabulary, word_to_index, config['oov_handling_method']), batch_size=config['batch_size'], collate_fn=collate_function)\n",
        "        test_loader = DataLoader(TextDataset(test_dataset, vocabulary, word_to_index, config['oov_handling_method']), batch_size=config['batch_size'], collate_fn=collate_function)\n",
        "\n",
        "        device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "        model.to(device)\n",
        "        \n",
        "        # Train the model\n",
        "        train_loss, val_accuracy, test_accuracy = train_model(\n",
        "            model=model,\n",
        "            train_loader=train_loader,\n",
        "            valid_loader=valid_loader,\n",
        "            test_loader=test_loader,\n",
        "            optimizer=optimizer,\n",
        "            epochs=config['epochs'],\n",
        "            patience=config['patience'],\n",
        "            scheduler_step_size=3,\n",
        "            scheduler_gamma=0.1,\n",
        "            device=device\n",
        "        )\n",
        "        \n",
        "\n",
        "    # Define CSV header\n",
        "    header = list(param_grid.keys()) + ['Epoch', 'Train Loss', 'Validation Accuracy', 'Test Accuracy']\n",
        "\n",
        "    # Write results to a CSV file\n",
        "    with open('part3_rnn_results.csv', mode='w', newline='') as csvfile:\n",
        "        writer = csv.DictWriter(csvfile, fieldnames=header)\n",
        "        writer.writeheader()\n",
        "        writer.writerows(results)\n",
        "\n",
        "    print(\"Parameter combinations and performances recorded in results.csv\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Function to run experiments from CSV\n",
        "def run_part3_blstm_bgru_experiments_from_paramgrid(train_dataset, validation_dataset, test_dataset):\n",
        "    param_combinations = list(itertools.product(*part_3_blstm_bgru_param_grid.values())) # Initialize a list to hold results\n",
        "    results = [] # Initialize a list to hold results\n",
        "    \n",
        "    # Iterate through each combination of parameters\n",
        "    for param in param_combinations:\n",
        "        config = dict(zip(part_3_blstm_bgru_param_grid.keys(), param))\n",
        "\n",
        "        # Load GloVe embeddings\n",
        "        glove_embeddings = load_glove_embeddings(config['glove_file_path'])\n",
        "\n",
        "        # Build vocabulary and create embedding matrix\n",
        "        vocabulary = build_vocabulary(train_dataset, config['oov_handling_method'])\n",
        "        embedding_matrix = create_embedding_matrix(config['embedding_dim'], vocabulary, glove_embeddings, config['oov_handling_method'])\n",
        "\n",
        "        # Build word_to_index mapping\n",
        "        word_to_index = {word: idx for idx, word in enumerate(vocabulary)}\n",
        "\n",
        "        # Initialize the model based on 'model_type'\n",
        "        if config['model_type'] == 'RNN':\n",
        "            model = SentimentRNN(\n",
        "                embedding_matrix=embedding_matrix,\n",
        "                hidden_size=config['hidden_size'],\n",
        "                output_size=config['output_size'],\n",
        "                rnn_type=config['rnn_type'],\n",
        "                num_layers=config['num_layers'],\n",
        "                use_bidirectional=config['use_bidirectional'],\n",
        "                use_dropout=config['use_dropout'],\n",
        "                use_batch_norm=config['use_batch_norm'],\n",
        "                use_layer_norm=config['use_layer_norm'],\n",
        "                aggregation_method=config['aggregation_method'],\n",
        "                freeze_embeddings=config['freeze_embeddings']\n",
        "            )\n",
        "            collate_function = collate_fn\n",
        "        else:\n",
        "            raise ValueError(f\"Unknown model type: {config['model_type']}\")\n",
        "\n",
        "        # Get the optimizer dynamically based on the config\n",
        "        optimizer_params = {\n",
        "            'optimizer_type': config['optimizer_type'],\n",
        "            'learning_rate': config['learning_rate'],\n",
        "            'momentum': config['momentum'],\n",
        "            'weight_decay': config['weight_decay']\n",
        "        }\n",
        "        optimizer = get_optimizer(optimizer_params, model)\n",
        "\n",
        "        # Create DataLoaders using the appropriate collate function\n",
        "        train_loader = DataLoader(TextDataset(train_dataset, vocabulary, word_to_index, config['oov_handling_method']), batch_size=config['batch_size'], shuffle=True, collate_fn=collate_function)\n",
        "        valid_loader = DataLoader(TextDataset(validation_dataset, vocabulary, word_to_index, config['oov_handling_method']), batch_size=config['batch_size'], collate_fn=collate_function)\n",
        "        test_loader = DataLoader(TextDataset(test_dataset, vocabulary, word_to_index, config['oov_handling_method']), batch_size=config['batch_size'], collate_fn=collate_function)\n",
        "\n",
        "        device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "        model.to(device)\n",
        "        \n",
        "        # Train the model\n",
        "        train_loss, val_accuracy, test_accuracy = train_model(\n",
        "            model=model,\n",
        "            train_loader=train_loader,\n",
        "            valid_loader=valid_loader,\n",
        "            test_loader=test_loader,\n",
        "            optimizer=optimizer,\n",
        "            epochs=config['epochs'],\n",
        "            patience=config['patience'],\n",
        "            scheduler_step_size=3,\n",
        "            scheduler_gamma=0.1,\n",
        "            device=device\n",
        "        )\n",
        "\n",
        "    # Define CSV header\n",
        "    header = list(param_grid.keys()) + ['Epoch', 'Train Loss', 'Validation Accuracy', 'Test Accuracy']\n",
        "\n",
        "    # Write results to a CSV file\n",
        "    with open('part3_bsltm_bgru_results.csv', mode='w', newline='') as csvfile:\n",
        "        writer = csv.DictWriter(csvfile, fieldnames=header)\n",
        "        writer.writeheader()\n",
        "        writer.writerows(results)\n",
        "\n",
        "    print(\"Parameter combinations and performances recorded in results.csv\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Function to run experiments from CSV\n",
        "def run_cnn_experiments_from_paramgrid(train_dataset, validation_dataset, test_dataset):\n",
        "    param_combinations = list(itertools.product(*part_3_cnn_param_grid.values())) # Initialize a list to hold results\n",
        "    results = [] # Initialize a list to hold results\n",
        "    \n",
        "    # Iterate through each combination of parameters\n",
        "    for param in param_combinations:\n",
        "        config = dict(zip(part_3_cnn_param_grid.keys(), param))\n",
        "\n",
        "        # Load GloVe embeddings\n",
        "        glove_embeddings = load_glove_embeddings(config['glove_file_path'])\n",
        "\n",
        "        # Build vocabulary and create embedding matrix\n",
        "        vocabulary = build_vocabulary(train_dataset, config['oov_handling_method'])\n",
        "        embedding_matrix = create_embedding_matrix(config['embedding_dim'], vocabulary, glove_embeddings, config['oov_handling_method'])\n",
        "\n",
        "        # Build word_to_index mapping\n",
        "        word_to_index = {word: idx for idx, word in enumerate(vocabulary)}\n",
        "\n",
        "        # Initialize the model based on 'model_type'\n",
        "        if config['model_type'] == 'CNN':\n",
        "            model = SentimentCNN(\n",
        "                embedding_matrix=embedding_matrix,\n",
        "                output_size=config['output_size'],\n",
        "                freeze_embeddings=config['freeze_embeddings'],\n",
        "                num_filters=config['num_filters'],\n",
        "                filter_sizes=config['filter_sizes'],\n",
        "                dropout_rate=config['dropout_rate']\n",
        "            )\n",
        "            collate_function = collate_fn_cnn\n",
        "        else:\n",
        "            raise ValueError(f\"Unknown model type: {config['model_type']}\")\n",
        "\n",
        "        # Get the optimizer dynamically based on the config\n",
        "        optimizer_params = {\n",
        "            'optimizer_type': config['optimizer_type'],\n",
        "            'learning_rate': config['learning_rate'],\n",
        "            'momentum': config['momentum'],\n",
        "            'weight_decay': config['weight_decay']\n",
        "        }\n",
        "        optimizer = get_optimizer(optimizer_params, model)\n",
        "\n",
        "        # Create DataLoaders using the appropriate collate function\n",
        "        train_loader = DataLoader(TextDataset(train_dataset, vocabulary, word_to_index, config['oov_handling_method']), batch_size=config['batch_size'], shuffle=True, collate_fn=collate_function)\n",
        "        valid_loader = DataLoader(TextDataset(validation_dataset, vocabulary, word_to_index, config['oov_handling_method']), batch_size=config['batch_size'], collate_fn=collate_function)\n",
        "        test_loader = DataLoader(TextDataset(test_dataset, vocabulary, word_to_index, config['oov_handling_method']), batch_size=config['batch_size'], collate_fn=collate_function)\n",
        "\n",
        "        device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "        model.to(device)\n",
        "        \n",
        "        # Train the model\n",
        "        train_loss, val_accuracy, test_accuracy = train_model(\n",
        "            model=model,\n",
        "            train_loader=train_loader,\n",
        "            valid_loader=valid_loader,\n",
        "            test_loader=test_loader,\n",
        "            optimizer=optimizer,\n",
        "            epochs=config['epochs'],\n",
        "            patience=config['patience'],\n",
        "            scheduler_step_size=3,\n",
        "            scheduler_gamma=0.1,\n",
        "            device=device\n",
        "        )\n",
        "\n",
        "    # Define CSV header\n",
        "    header = list(param_grid.keys()) + ['Epoch', 'Train Loss', 'Validation Accuracy', 'Test Accuracy']\n",
        "\n",
        "    # Write results to a CSV file\n",
        "    with open('part3_cnn_results.csv', mode='w', newline='') as csvfile:\n",
        "        writer = csv.DictWriter(csvfile, fieldnames=header)\n",
        "        writer.writeheader()\n",
        "        writer.writerows(results)\n",
        "\n",
        "    print(\"Parameter combinations and performances recorded in results.csv\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5xFaeosRZRSQ",
        "outputId": "260e9e8c-783d-4775-9e90-217e00161c3e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'optimizer_type': 'Adam', 'learning_rate': 0.01, 'momentum': 0, 'weight_decay': 0.0001, 'batch_size': 32, 'epochs': 100, 'patience': 10, 'model_type': 'RNN', 'rnn_type': 'RNN', 'num_layers': 1, 'use_bidirectional': False, 'use_dropout': True, 'use_batch_norm': True, 'use_layer_norm': True, 'aggregation_method': 'last_hidden', 'hidden_size': 64, 'output_size': 2, 'freeze_embeddings': True, 'oov_handling_method': 'none', 'embedding_dim': 100, 'glove_file_path': 'glove.6B.100d.txt'}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/var/folders/g7/dzzn64hs1xldzfp9ks8cdpq40000gn/T/ipykernel_7724/1654731144.py:4: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  inputs = [torch.tensor(x) for x in inputs]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "best_epoch:  2\n",
            "best_epoch:  5\n",
            "Epoch 10, Loss: 0.6931, Validation Accuracy: 49.81%\n",
            "Early stopping at epoch 15\n"
          ]
        },
        {
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: 'best_model_4.pt'",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[31], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# run_experiments_from_csv(train_dataset, validation_dataset, test_dataset)\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[43mrun_part2_rnn_experiments_from_paramgrid\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_dataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalidation_dataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_dataset\u001b[49m\u001b[43m)\u001b[49m\n",
            "Cell \u001b[0;32mIn[27], line 58\u001b[0m, in \u001b[0;36mrun_part2_rnn_experiments_from_paramgrid\u001b[0;34m(train_dataset, validation_dataset, test_dataset)\u001b[0m\n\u001b[1;32m     55\u001b[0m model\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m     57\u001b[0m \u001b[38;5;66;03m# Train the model\u001b[39;00m\n\u001b[0;32m---> 58\u001b[0m train_loss, val_accuracy, test_accuracy \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     59\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     60\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     61\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvalid_loader\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvalid_loader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     62\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtest_loader\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtest_loader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     63\u001b[0m \u001b[43m    \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     64\u001b[0m \u001b[43m    \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mepochs\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     65\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpatience\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mpatience\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     66\u001b[0m \u001b[43m    \u001b[49m\u001b[43mscheduler_step_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     67\u001b[0m \u001b[43m    \u001b[49m\u001b[43mscheduler_gamma\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.1\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     68\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice\u001b[49m\n\u001b[1;32m     69\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     71\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(train_loss)):  \u001b[38;5;66;03m# Iterate through each epoch's results\u001b[39;00m\n\u001b[1;32m     72\u001b[0m     config[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mEpoch\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m epoch\n",
            "Cell \u001b[0;32mIn[20], line 53\u001b[0m, in \u001b[0;36mtrain_model\u001b[0;34m(model, train_loader, valid_loader, test_loader, optimizer, epochs, patience, scheduler_step_size, scheduler_gamma, device)\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[38;5;66;03m# Load the best model before evaluating on test set\u001b[39;00m\n\u001b[1;32m     52\u001b[0m best_epoch \u001b[38;5;241m=\u001b[39m val_accuracies\u001b[38;5;241m.\u001b[39mindex(best_val_accuracy) \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m---> 53\u001b[0m model\u001b[38;5;241m.\u001b[39mload_state_dict(\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mbest_model_\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mbest_epoch\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m.pt\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m)\n\u001b[1;32m     55\u001b[0m \u001b[38;5;66;03m# Test Accuracy\u001b[39;00m\n\u001b[1;32m     56\u001b[0m test_accuracy \u001b[38;5;241m=\u001b[39m evaluate_accuracy(model, test_loader, device)\n",
            "File \u001b[0;32m/opt/homebrew/lib/python3.9/site-packages/torch/serialization.py:997\u001b[0m, in \u001b[0;36mload\u001b[0;34m(f, map_location, pickle_module, weights_only, mmap, **pickle_load_args)\u001b[0m\n\u001b[1;32m    994\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mencoding\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m pickle_load_args\u001b[38;5;241m.\u001b[39mkeys():\n\u001b[1;32m    995\u001b[0m     pickle_load_args[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mencoding\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m--> 997\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[43m_open_file_like\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mrb\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m opened_file:\n\u001b[1;32m    998\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m _is_zipfile(opened_file):\n\u001b[1;32m    999\u001b[0m         \u001b[38;5;66;03m# The zipfile reader is going to advance the current file position.\u001b[39;00m\n\u001b[1;32m   1000\u001b[0m         \u001b[38;5;66;03m# If we want to actually tail call to torch.jit.load, we need to\u001b[39;00m\n\u001b[1;32m   1001\u001b[0m         \u001b[38;5;66;03m# reset back to the original position.\u001b[39;00m\n\u001b[1;32m   1002\u001b[0m         orig_position \u001b[38;5;241m=\u001b[39m opened_file\u001b[38;5;241m.\u001b[39mtell()\n",
            "File \u001b[0;32m/opt/homebrew/lib/python3.9/site-packages/torch/serialization.py:444\u001b[0m, in \u001b[0;36m_open_file_like\u001b[0;34m(name_or_buffer, mode)\u001b[0m\n\u001b[1;32m    442\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_open_file_like\u001b[39m(name_or_buffer, mode):\n\u001b[1;32m    443\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m _is_path(name_or_buffer):\n\u001b[0;32m--> 444\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_open_file\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    445\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    446\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mw\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01min\u001b[39;00m mode:\n",
            "File \u001b[0;32m/opt/homebrew/lib/python3.9/site-packages/torch/serialization.py:425\u001b[0m, in \u001b[0;36m_open_file.__init__\u001b[0;34m(self, name, mode)\u001b[0m\n\u001b[1;32m    424\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, name, mode):\n\u001b[0;32m--> 425\u001b[0m     \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m)\u001b[49m)\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'best_model_4.pt'"
          ]
        }
      ],
      "source": [
        "# run_experiments_from_csv(train_dataset, validation_dataset, test_dataset)\n",
        "run_part2_rnn_experiments_from_paramgrid(train_dataset, validation_dataset, test_dataset)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "r1BNKjKJmco_"
      },
      "outputs": [],
      "source": [
        "#run_part3_rnn_experiments_from_paramgrid(train_dataset, validation_dataset, test_dataset)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#run_part3_blstm_bgru_experiments_from_paramgrid(train_dataset, validation_dataset, test_dataset)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#run_cnn_experiments_from_paramgrid(train_dataset, validation_dataset, test_dataset)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
