{"cells":[{"cell_type":"markdown","metadata":{"id":"view-in-github"},"source":["<a href=\"https://colab.research.google.com/github/Abeszz/SC4002-NLP-Assignment/blob/main/SC4002.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"]},{"cell_type":"markdown","metadata":{"id":"fRPGfwCv_acw"},"source":["# Installation & Requirements :"]},{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"collapsed":true,"executionInfo":{"elapsed":6460,"status":"ok","timestamp":1729224240183,"user":{"displayName":"Amirul Hakim","userId":"06358676218988637945"},"user_tz":-480},"id":"J81YDyhL_mX5","outputId":"c5078ff9-db54-4d96-ec2f-e99a18d48e82"},"outputs":[{"name":"stdout","output_type":"stream","text":["Collecting datasetsNote: you may need to restart the kernel to use updated packages.\n","\n","  Downloading datasets-3.0.1-py3-none-any.whl.metadata (20 kB)\n","Collecting filelock (from datasets)\n","  Downloading filelock-3.16.1-py3-none-any.whl.metadata (2.9 kB)\n","Collecting numpy>=1.17 (from datasets)\n","  Downloading numpy-2.1.2-cp312-cp312-win_amd64.whl.metadata (59 kB)\n","Collecting pyarrow>=15.0.0 (from datasets)\n","  Downloading pyarrow-17.0.0-cp312-cp312-win_amd64.whl.metadata (3.4 kB)\n","Collecting dill<0.3.9,>=0.3.0 (from datasets)\n","  Downloading dill-0.3.8-py3-none-any.whl.metadata (10 kB)\n","Collecting pandas (from datasets)\n","  Downloading pandas-2.2.3-cp312-cp312-win_amd64.whl.metadata (19 kB)\n","Collecting requests>=2.32.2 (from datasets)\n","  Downloading requests-2.32.3-py3-none-any.whl.metadata (4.6 kB)\n","Collecting tqdm>=4.66.3 (from datasets)\n","  Downloading tqdm-4.66.5-py3-none-any.whl.metadata (57 kB)\n","Collecting xxhash (from datasets)\n","  Downloading xxhash-3.5.0-cp312-cp312-win_amd64.whl.metadata (13 kB)\n","Collecting multiprocess (from datasets)\n","  Downloading multiprocess-0.70.17-py312-none-any.whl.metadata (7.2 kB)\n","Collecting fsspec<=2024.6.1,>=2023.1.0 (from fsspec[http]<=2024.6.1,>=2023.1.0->datasets)\n","  Downloading fsspec-2024.6.1-py3-none-any.whl.metadata (11 kB)\n","Collecting aiohttp (from datasets)\n","  Downloading aiohttp-3.10.10-cp312-cp312-win_amd64.whl.metadata (7.8 kB)\n","Collecting huggingface-hub>=0.22.0 (from datasets)\n","  Downloading huggingface_hub-0.25.2-py3-none-any.whl.metadata (13 kB)\n","Requirement already satisfied: packaging in c:\\users\\hakim.amirul\\appdata\\roaming\\python\\python312\\site-packages (from datasets) (24.1)\n","Collecting pyyaml>=5.1 (from datasets)\n","  Downloading PyYAML-6.0.2-cp312-cp312-win_amd64.whl.metadata (2.1 kB)\n","Collecting aiohappyeyeballs>=2.3.0 (from aiohttp->datasets)\n","  Downloading aiohappyeyeballs-2.4.3-py3-none-any.whl.metadata (6.1 kB)\n","Collecting aiosignal>=1.1.2 (from aiohttp->datasets)\n","  Downloading aiosignal-1.3.1-py3-none-any.whl.metadata (4.0 kB)\n","Collecting attrs>=17.3.0 (from aiohttp->datasets)\n","  Downloading attrs-24.2.0-py3-none-any.whl.metadata (11 kB)\n","Collecting frozenlist>=1.1.1 (from aiohttp->datasets)\n","  Downloading frozenlist-1.4.1-cp312-cp312-win_amd64.whl.metadata (12 kB)\n","Collecting multidict<7.0,>=4.5 (from aiohttp->datasets)\n","  Downloading multidict-6.1.0-cp312-cp312-win_amd64.whl.metadata (5.1 kB)\n","Collecting yarl<2.0,>=1.12.0 (from aiohttp->datasets)\n","  Downloading yarl-1.15.4-cp312-cp312-win_amd64.whl.metadata (60 kB)\n","Collecting typing-extensions>=3.7.4.3 (from huggingface-hub>=0.22.0->datasets)\n","  Downloading typing_extensions-4.12.2-py3-none-any.whl.metadata (3.0 kB)\n","Collecting charset-normalizer<4,>=2 (from requests>=2.32.2->datasets)\n","  Downloading charset_normalizer-3.4.0-cp312-cp312-win_amd64.whl.metadata (34 kB)\n","Collecting idna<4,>=2.5 (from requests>=2.32.2->datasets)\n","  Downloading idna-3.10-py3-none-any.whl.metadata (10 kB)\n","Collecting urllib3<3,>=1.21.1 (from requests>=2.32.2->datasets)\n","  Downloading urllib3-2.2.3-py3-none-any.whl.metadata (6.5 kB)\n","Collecting certifi>=2017.4.17 (from requests>=2.32.2->datasets)\n","  Downloading certifi-2024.8.30-py3-none-any.whl.metadata (2.2 kB)\n","Requirement already satisfied: colorama in c:\\users\\hakim.amirul\\appdata\\roaming\\python\\python312\\site-packages (from tqdm>=4.66.3->datasets) (0.4.6)\n","INFO: pip is looking at multiple versions of multiprocess to determine which version is compatible with other requirements. This could take a while.\n","Collecting multiprocess (from datasets)\n","  Downloading multiprocess-0.70.16-py312-none-any.whl.metadata (7.2 kB)\n","Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\hakim.amirul\\appdata\\roaming\\python\\python312\\site-packages (from pandas->datasets) (2.9.0.post0)\n","Collecting pytz>=2020.1 (from pandas->datasets)\n","  Downloading pytz-2024.2-py2.py3-none-any.whl.metadata (22 kB)\n","Collecting tzdata>=2022.7 (from pandas->datasets)\n","  Downloading tzdata-2024.2-py2.py3-none-any.whl.metadata (1.4 kB)\n","Requirement already satisfied: six>=1.5 in c:\\users\\hakim.amirul\\appdata\\roaming\\python\\python312\\site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n","Collecting propcache>=0.2.0 (from yarl<2.0,>=1.12.0->aiohttp->datasets)\n","  Downloading propcache-0.2.0-cp312-cp312-win_amd64.whl.metadata (7.9 kB)\n","Downloading datasets-3.0.1-py3-none-any.whl (471 kB)\n","Downloading dill-0.3.8-py3-none-any.whl (116 kB)\n","Downloading fsspec-2024.6.1-py3-none-any.whl (177 kB)\n","Downloading aiohttp-3.10.10-cp312-cp312-win_amd64.whl (379 kB)\n","Downloading huggingface_hub-0.25.2-py3-none-any.whl (436 kB)\n","Downloading numpy-2.1.2-cp312-cp312-win_amd64.whl (12.6 MB)\n","   ---------------------------------------- 0.0/12.6 MB ? eta -:--:--\n","   ---------------------------------------  12.3/12.6 MB 70.1 MB/s eta 0:00:01\n","   ---------------------------------------- 12.6/12.6 MB 49.2 MB/s eta 0:00:00\n","Downloading pyarrow-17.0.0-cp312-cp312-win_amd64.whl (25.1 MB)\n","   ---------------------------------------- 0.0/25.1 MB ? eta -:--:--\n","   ---------------------------- ----------- 17.8/25.1 MB 86.5 MB/s eta 0:00:01\n","   ---------------------------------------- 25.1/25.1 MB 61.2 MB/s eta 0:00:00\n","Downloading PyYAML-6.0.2-cp312-cp312-win_amd64.whl (156 kB)\n","Downloading requests-2.32.3-py3-none-any.whl (64 kB)\n","Downloading tqdm-4.66.5-py3-none-any.whl (78 kB)\n","Downloading filelock-3.16.1-py3-none-any.whl (16 kB)\n","Downloading multiprocess-0.70.16-py312-none-any.whl (146 kB)\n","Downloading pandas-2.2.3-cp312-cp312-win_amd64.whl (11.5 MB)\n","   ---------------------------------------- 0.0/11.5 MB ? eta -:--:--\n","   ---------------------------------------  11.3/11.5 MB 70.6 MB/s eta 0:00:01\n","   ---------------------------------------- 11.5/11.5 MB 51.3 MB/s eta 0:00:00\n","Downloading xxhash-3.5.0-cp312-cp312-win_amd64.whl (30 kB)\n","Downloading aiohappyeyeballs-2.4.3-py3-none-any.whl (14 kB)\n","Downloading aiosignal-1.3.1-py3-none-any.whl (7.6 kB)\n","Downloading attrs-24.2.0-py3-none-any.whl (63 kB)\n","Downloading certifi-2024.8.30-py3-none-any.whl (167 kB)\n","Downloading charset_normalizer-3.4.0-cp312-cp312-win_amd64.whl (102 kB)\n","Downloading frozenlist-1.4.1-cp312-cp312-win_amd64.whl (50 kB)\n","Downloading idna-3.10-py3-none-any.whl (70 kB)\n","Downloading multidict-6.1.0-cp312-cp312-win_amd64.whl (28 kB)\n","Downloading pytz-2024.2-py2.py3-none-any.whl (508 kB)\n","Downloading typing_extensions-4.12.2-py3-none-any.whl (37 kB)\n","Downloading tzdata-2024.2-py2.py3-none-any.whl (346 kB)\n","Downloading urllib3-2.2.3-py3-none-any.whl (126 kB)\n","Downloading yarl-1.15.4-cp312-cp312-win_amd64.whl (84 kB)\n","Downloading propcache-0.2.0-cp312-cp312-win_amd64.whl (44 kB)\n","Installing collected packages: pytz, xxhash, urllib3, tzdata, typing-extensions, tqdm, pyyaml, propcache, numpy, multidict, idna, fsspec, frozenlist, filelock, dill, charset-normalizer, certifi, attrs, aiohappyeyeballs, yarl, requests, pyarrow, pandas, multiprocess, aiosignal, huggingface-hub, aiohttp, datasets\n","Successfully installed aiohappyeyeballs-2.4.3 aiohttp-3.10.10 aiosignal-1.3.1 attrs-24.2.0 certifi-2024.8.30 charset-normalizer-3.4.0 datasets-3.0.1 dill-0.3.8 filelock-3.16.1 frozenlist-1.4.1 fsspec-2024.6.1 huggingface-hub-0.25.2 idna-3.10 multidict-6.1.0 multiprocess-0.70.16 numpy-2.1.2 pandas-2.2.3 propcache-0.2.0 pyarrow-17.0.0 pytz-2024.2 pyyaml-6.0.2 requests-2.32.3 tqdm-4.66.5 typing-extensions-4.12.2 tzdata-2024.2 urllib3-2.2.3 xxhash-3.5.0 yarl-1.15.4\n"]},{"name":"stderr","output_type":"stream","text":["  WARNING: The script tqdm.exe is installed in 'c:\\Users\\hakim.amirul\\AppData\\Local\\Programs\\Python\\Python312\\Scripts' which is not on PATH.\n","  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\n","  WARNING: The scripts f2py.exe and numpy-config.exe are installed in 'c:\\Users\\hakim.amirul\\AppData\\Local\\Programs\\Python\\Python312\\Scripts' which is not on PATH.\n","  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\n","  WARNING: The script normalizer.exe is installed in 'c:\\Users\\hakim.amirul\\AppData\\Local\\Programs\\Python\\Python312\\Scripts' which is not on PATH.\n","  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\n","  WARNING: The script huggingface-cli.exe is installed in 'c:\\Users\\hakim.amirul\\AppData\\Local\\Programs\\Python\\Python312\\Scripts' which is not on PATH.\n","  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\n","  WARNING: The script datasets-cli.exe is installed in 'c:\\Users\\hakim.amirul\\AppData\\Local\\Programs\\Python\\Python312\\Scripts' which is not on PATH.\n","  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\n"]}],"source":["pip install datasets"]},{"cell_type":"code","execution_count":2,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"collapsed":true,"executionInfo":{"elapsed":8609,"status":"ok","timestamp":1729224248789,"user":{"displayName":"Amirul Hakim","userId":"06358676218988637945"},"user_tz":-480},"id":"ZZ7IipO2Bmo1","outputId":"55a20d47-00a6-42be-e72a-9b6cdda7e8d4"},"outputs":[{"name":"stdout","output_type":"stream","text":["Collecting nltkNote: you may need to restart the kernel to use updated packages.\n"]},{"name":"stderr","output_type":"stream","text":["  WARNING: The script nltk.exe is installed in 'c:\\Users\\hakim.amirul\\AppData\\Local\\Programs\\Python\\Python312\\Scripts' which is not on PATH.\n","  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\n"]},{"name":"stdout","output_type":"stream","text":["\n","  Downloading nltk-3.9.1-py3-none-any.whl.metadata (2.9 kB)\n","Collecting click (from nltk)\n","  Downloading click-8.1.7-py3-none-any.whl.metadata (3.0 kB)\n","Collecting joblib (from nltk)\n","  Downloading joblib-1.4.2-py3-none-any.whl.metadata (5.4 kB)\n","Collecting regex>=2021.8.3 (from nltk)\n","  Downloading regex-2024.9.11-cp312-cp312-win_amd64.whl.metadata (41 kB)\n","Requirement already satisfied: tqdm in c:\\users\\hakim.amirul\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from nltk) (4.66.5)\n","Requirement already satisfied: colorama in c:\\users\\hakim.amirul\\appdata\\roaming\\python\\python312\\site-packages (from click->nltk) (0.4.6)\n","Downloading nltk-3.9.1-py3-none-any.whl (1.5 MB)\n","   ---------------------------------------- 0.0/1.5 MB ? eta -:--:--\n","   ---------------------------------------- 1.5/1.5 MB 77.7 MB/s eta 0:00:00\n","Downloading regex-2024.9.11-cp312-cp312-win_amd64.whl (273 kB)\n","Downloading click-8.1.7-py3-none-any.whl (97 kB)\n","Downloading joblib-1.4.2-py3-none-any.whl (301 kB)\n","Installing collected packages: regex, joblib, click, nltk\n","Successfully installed click-8.1.7 joblib-1.4.2 nltk-3.9.1 regex-2024.9.11\n"]}],"source":["pip install nltk"]},{"cell_type":"code","execution_count":3,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"collapsed":true,"executionInfo":{"elapsed":5410,"status":"ok","timestamp":1729224254178,"user":{"displayName":"Amirul Hakim","userId":"06358676218988637945"},"user_tz":-480},"id":"wzYNgiwEBtoL","outputId":"f9c1cf35-2dd1-411b-aaf8-30e14a1fb596"},"outputs":[{"name":"stdout","output_type":"stream","text":["Requirement already satisfied: numpy in c:\\users\\hakim.amirul\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (2.1.2)\n","Note: you may need to restart the kernel to use updated packages.\n"]}],"source":["pip install numpy"]},{"cell_type":"code","execution_count":4,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"collapsed":true,"executionInfo":{"elapsed":5614,"status":"ok","timestamp":1729224259771,"user":{"displayName":"Amirul Hakim","userId":"06358676218988637945"},"user_tz":-480},"id":"3pFMpM5qpLFn","outputId":"beea3d50-26e7-4979-a99b-b88bb927daf6"},"outputs":[{"name":"stdout","output_type":"stream","text":["Collecting npm\n","  Downloading npm-0.1.1.tar.gz (2.5 kB)\n","  Installing build dependencies: started\n","  Installing build dependencies: finished with status 'done'\n","  Getting requirements to build wheel: started\n","  Getting requirements to build wheel: finished with status 'done'\n","  Preparing metadata (pyproject.toml): started\n","  Preparing metadata (pyproject.toml): finished with status 'done'\n","Collecting optional-django==0.1.0 (from npm)\n","  Downloading optional-django-0.1.0.tar.gz (9.5 kB)\n","  Installing build dependencies: started\n","  Installing build dependencies: finished with status 'done'\n","  Getting requirements to build wheel: started\n","  Getting requirements to build wheel: finished with status 'done'\n","  Preparing metadata (pyproject.toml): started\n","  Preparing metadata (pyproject.toml): finished with status 'done'\n","Building wheels for collected packages: npm, optional-django\n","  Building wheel for npm (pyproject.toml): started\n","  Building wheel for npm (pyproject.toml): finished with status 'done'\n","  Created wheel for npm: filename=npm-0.1.1-py3-none-any.whl size=3692 sha256=602309f02d0ba542d8158d21aee033df8d7ac2a7417d9632083b78d5e58335de\n","  Stored in directory: c:\\users\\hakim.amirul\\appdata\\local\\pip\\cache\\wheels\\04\\04\\bb\\36c9ed4d58bad70eebb1210104b60ccc93ccef4253cf5362c3\n","  Building wheel for optional-django (pyproject.toml): started\n","  Building wheel for optional-django (pyproject.toml): finished with status 'done'\n","  Created wheel for optional-django: filename=optional_django-0.1.0-py3-none-any.whl size=9959 sha256=b12eae3113e85dbf7d4efa35ed10e54a77f3ff2198fd1bb4a15569abc762f75c\n","  Stored in directory: c:\\users\\hakim.amirul\\appdata\\local\\pip\\cache\\wheels\\4c\\b2\\fe\\5a41dada2276b894f5f41b2850eab1a1862492b222bfd02e75\n","Successfully built npm optional-django\n","Installing collected packages: optional-django, npm\n","Successfully installed npm-0.1.1 optional-django-0.1.0\n","Note: you may need to restart the kernel to use updated packages.\n"]}],"source":["pip install npm"]},{"cell_type":"code","execution_count":5,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":10606,"status":"ok","timestamp":1729224270354,"user":{"displayName":"Amirul Hakim","userId":"06358676218988637945"},"user_tz":-480},"id":"0nqNXGOPywf8","outputId":"839f06c3-2a28-4a8a-fa12-7730f3d9df71"},"outputs":[{"name":"stderr","output_type":"stream","text":["'pip' is not recognized as an internal or external command,\n","operable program or batch file.\n"]}],"source":["!pip install torch"]},{"cell_type":"code","execution_count":6,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"collapsed":true,"id":"IFIJIoS9DuVa","outputId":"fd82a812-7c4e-4ab0-f3df-e78893c08e44"},"outputs":[{"name":"stderr","output_type":"stream","text":["'wget' is not recognized as an internal or external command,\n","operable program or batch file.\n","'unzip' is not recognized as an internal or external command,\n","operable program or batch file.\n"]}],"source":["!wget https://nlp.stanford.edu/data/glove.6B.zip\n","!unzip glove.6B.zip"]},{"cell_type":"code","execution_count":7,"metadata":{"collapsed":true,"id":"B8qRXCT2GpKd"},"outputs":[{"name":"stderr","output_type":"stream","text":["[nltk_data] Downloading package punkt to c:\\Users\\hakim.amirul\\AppData\n","[nltk_data]     \\Local\\Programs\\Python\\Python312\\nltk_data...\n","[nltk_data]   Unzipping tokenizers\\punkt.zip.\n","[nltk_data] Downloading package punkt_tab to c:\\Users\\hakim.amirul\\App\n","[nltk_data]     Data\\Local\\Programs\\Python\\Python312\\nltk_data...\n","[nltk_data]   Unzipping tokenizers\\punkt_tab.zip.\n"]},{"data":{"text/plain":["True"]},"execution_count":7,"metadata":{},"output_type":"execute_result"}],"source":["import os\n","import nltk\n","import sys\n","\n","env_base_path = sys.prefix\n","nltk_path = os.path.join(env_base_path, 'nltk_data')\n","nltk.download('punkt', nltk_path)\n","nltk.download('punkt_tab', nltk_path)"]},{"cell_type":"markdown","metadata":{"id":"PEtxZ6Hf9ezr"},"source":["# **PART 0. Dataset Preparation**"]},{"cell_type":"code","execution_count":8,"metadata":{"id":"iBA7sNlf9hW5"},"outputs":[{"name":"stderr","output_type":"stream","text":["c:\\Users\\hakim.amirul\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n","  from .autonotebook import tqdm as notebook_tqdm\n","c:\\Users\\hakim.amirul\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\huggingface_hub\\file_download.py:147: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\hakim.amirul\\.cache\\huggingface\\hub\\datasets--rotten_tomatoes. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n","To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n","  warnings.warn(message)\n","Generating train split: 100%|██████████| 8530/8530 [00:00<00:00, 155090.05 examples/s]\n","Generating validation split: 100%|██████████| 1066/1066 [00:00<00:00, 177693.67 examples/s]\n","Generating test split: 100%|██████████| 1066/1066 [00:00<00:00, 177700.73 examples/s]\n"]}],"source":["from datasets import load_dataset\n","dataset = load_dataset('rotten_tomatoes')"]},{"cell_type":"code","execution_count":9,"metadata":{"id":"o_TvlwmgB35t"},"outputs":[],"source":["train_dataset = dataset['train']\n","validation_dataset = dataset['validation']\n","test_dataset = dataset['test']"]},{"cell_type":"markdown","metadata":{"id":"y-2g6fSkCYzh"},"source":["# **Part 1. Preparing Word Embedding**"]},{"cell_type":"code","execution_count":10,"metadata":{"id":"cBY500zfFszo"},"outputs":[],"source":["vocabulary = None\n","vocab_size = None\n","embedding_matrix = None\n","oov_words = [] # Array to track OOV words\n","UNKNOWN_TOKEN = '<UNKNOWN>'  # Special token for OOV words\n","\n","glove_model_file = 'glove.6B.300d.txt'\n","glove_model_dimension = 300"]},{"cell_type":"code","execution_count":11,"metadata":{"id":"ISeeey8cChMl"},"outputs":[],"source":["from nltk.tokenize import word_tokenize\n","from collections import Counter\n","\n","def tokenize_sentence(sentence):\n","    # Tokenize each sentence using NLTK's word_tokenize\n","    return word_tokenize(sentence.lower()) # Convert to lowercase for consistency\n","\n","def tokenize_dataset(dataset):\n","    global vocabulary, vocab_size\n","    # Build a vocabulary from the dataset\n","    vocab_counter = Counter()\n","\n","    for word in dataset:\n","        tokens = tokenize_sentence(word['text'])\n","        vocab_counter.update(tokens)\n","\n","    # Update the vocabulary list\n","    vocabulary = list(vocab_counter.keys())\n","    vocab_size = len(vocabulary)"]},{"cell_type":"code","execution_count":12,"metadata":{"id":"zx90AHM4CmyQ"},"outputs":[],"source":["def load_glove_embeddings(glove_file):\n","    embeddings_index = {}\n","    with open(glove_file, 'r', encoding='utf-8') as f:\n","        for line in f:\n","            values = line.split()\n","            word = values[0]\n","            vector = np.asarray(values[1:], dtype='float32')\n","            embeddings_index[word] = vector\n","    return embeddings_index"]},{"cell_type":"code","execution_count":13,"metadata":{"id":"1rf5y-YdFyJj"},"outputs":[],"source":["import numpy as np\n","\n","def create_embedding_matrix(glove_file_path, glove_model_dimension):\n","    global vocabulary, embedding_matrix, oov_words\n","    # Load the GloVe embeddings into a dictionary\n","    glove_embeddings = load_glove_embeddings(glove_file_path)\n","\n","    # Create the embedding matrix\n","    embedding_matrix = np.zeros((vocab_size, glove_model_dimension))  # Initialize the matrix with zeros\n","\n","    # Initialize the UNKNOWN token with a random vector\n","    unknown_vector = np.random.normal(scale=0.6, size=(glove_model_dimension,))\n","\n","    # Fill the embedding matrix\n","    for idx, word in enumerate(vocabulary):\n","        if word in glove_embeddings:\n","            embedding_matrix[idx] = glove_embeddings[word]  # Use the pretrained GloVe vector\n","        elif word == UNKNOWN_TOKEN:\n","            embedding_matrix[idx] = unknown_vector  # Do not add UNKNOWN token to oov\n","        else:\n","            oov_words.append(word)  # Track OOV words\n","            embedding_matrix[idx] = unknown_vector"]},{"cell_type":"code","execution_count":14,"metadata":{"id":"wZIIBlqXFzv5"},"outputs":[],"source":["def add_unkown_token():\n","    global vocabulary, vocab_size, UNKNOWN_TOKEN\n","\n","    # Check if 'UNK' is already in the vocabulary, if not, add it\n","    if UNKNOWN_TOKEN not in vocabulary:\n","        vocabulary.append(UNKNOWN_TOKEN)\n","        vocab_size += 1\n"]},{"cell_type":"code","execution_count":15,"metadata":{"id":"X1XXpi0cGKkH"},"outputs":[],"source":["# Tokenize the dataset\n","tokenize_dataset(train_dataset)"]},{"cell_type":"code","execution_count":16,"metadata":{"id":"KVj-jjMIG2u8"},"outputs":[{"name":"stdout","output_type":"stream","text":["1(a) Number of Vocabulary words in training dataset = 18029\n"]}],"source":["# Print the size of Vocabulary\n","print(f\"1(a) Number of Vocabulary words in training dataset = {vocab_size}\")"]},{"cell_type":"code","execution_count":17,"metadata":{"id":"nOBSpBhRIiPy"},"outputs":[],"source":["# 1(c) : Add <UNKNOWN> token and replace all OOV words with it\n","add_unkown_token()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"o4GZxuibF1xr"},"outputs":[],"source":["# Initialize the glove model with our configuration\n","create_embedding_matrix(glove_model_file, glove_model_dimension)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"7M8RViMQIzuz"},"outputs":[],"source":["# Print the number of OOV words\n","print(f\"1(b) Number of Out Of Vocabulary words: {len(oov_words)}\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"k_gcspQilFpM"},"outputs":[],"source":["import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","from torch.utils.data import DataLoader, Dataset\n","from torch.nn.utils.rnn import pad_sequence\n","from nltk.tokenize import word_tokenize\n","import json"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"fbyFvuyblJoJ"},"outputs":[],"source":["# Custom collate_fn to handle padding\n","def collate_fn(batch):\n","    inputs, labels = zip(*batch)\n","    inputs = [torch.tensor(x) for x in inputs]\n","    padded_inputs = pad_sequence(inputs, batch_first=True, padding_value=0)  # Pad sequences\n","    labels = torch.stack(labels)\n","    return padded_inputs, labels"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"mD2LgLYJnAKH"},"outputs":[],"source":["class SentimentRNN(nn.Module):\n","    def __init__(self, embedding_matrix, hidden_size, output_size, num_layers=1):\n","        super(SentimentRNN, self).__init__()\n","\n","        vocab_size, embedding_dim = embedding_matrix.shape\n","\n","        # Embedding layer using pre-trained embeddings\n","        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n","        self.embedding.weight = nn.Parameter(torch.tensor(embedding_matrix, dtype=torch.float32))\n","        self.embedding.weight.requires_grad = False  # freeze embeddings\n","\n","        # Simple RNN layer, 1 layer, uni-directional\n","        self.rnn = nn.RNN(embedding_dim, hidden_size, num_layers=num_layers, batch_first=True)  # Uni-directional RNN with 1 layer\n","\n","        # RNN layer\n","        # self.rnn = nn.RNN(embedding_dim, hidden_size, num_layers=2, batch_first=True, bidirectional=True)\n","        # self.rnn = nn.LSTM(embedding_dim, hidden_size, num_layers=2, batch_first=True, bidirectional=True)\n","        # self.rnn = nn.GRU(embedding_dim, hidden_size, num_layers=num_layers, batch_first=True, bidirectional=True)\n","\n","        # Fully connected layer for classification\n","        self.fc = nn.Linear(hidden_size, output_size)  # Double hidden size for bidirectional RNN\n","\n","        # Regularization layers\n","        # self.dropout = nn.Dropout(0.3)  # dropout rate\n","        # self.batch_norm = nn.BatchNorm1d(hidden_size * 2)  # Add batch normalization\n","        # self.layer_norm = nn.LayerNorm(hidden_size * 2)  # Add layer normalization\n","\n","    def forward(self, x):\n","        embedded = self.embedding(x)\n","        output, hidden = self.rnn(embedded) # for rnn and gru only two values are returned\n","        final_output = hidden[-1] # Use the last hidden state for uni-directional RNN\n","        # output, (hidden, _) = self.rnn(embedded) # for lstm\n","        # final_output = torch.cat((hidden[-2,:,:], hidden[-1,:,:]), dim=1)  # Concatenate forward and backward hidden states\n","        # final_output = self.batch_norm(final_output)  # Apply batch normalization\n","        # final_output = self.layer_norm(final_output)  # Apply layer normalization\n","        # final_output = self.dropout(final_output)     # Apply dropout\n","        return self.fc(final_output)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"rfQC1E58nDVQ"},"outputs":[],"source":["# Dataset class to handle data batching\n","class TextDataset(Dataset):\n","    def __init__(self, dataset, vocab):\n","        self.dataset = dataset\n","        self.vocab = vocab\n","\n","    def __len__(self):\n","        return len(self.dataset)\n","\n","    def __getitem__(self, idx):\n","        sentence = self.dataset[idx]['text']\n","        label = self.dataset[idx]['label']\n","        tokens = word_tokenize(sentence.lower())\n","        indices = [self.vocab.index(token) if token in self.vocab else self.vocab.index(\"<UNKNOWN>\") for token in tokens]\n","        return torch.tensor(indices), torch.tensor(label)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"iRnbJyuPnH6i"},"outputs":[],"source":["def get_optimizer(params, model):\n","    optimizer_type = params[\"optimizer_type\"]\n","    lr = params[\"learning_rate\"]\n","    weight_decay = params.get(\"weight_decay\", 0)  # Default to 0 if not specified\n","\n","    if optimizer_type == \"SGD\":\n","        momentum = params.get(\"momentum\", 0)  # Default to 0 if not specified\n","        return optim.SGD(model.parameters(), lr=lr, momentum=momentum, weight_decay=weight_decay)\n","\n","    elif optimizer_type == \"Adam\":\n","        return optim.Adam(model.parameters(), lr=lr, weight_decay=weight_decay)\n","\n","    else:\n","        raise ValueError(f\"Unknown optimizer type: {optimizer_type}\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ZrGk6FWznMpY"},"outputs":[],"source":["def train_model(train_data, valid_data, vocab, embedding_matrix, params_file=\"hyperparams.txt\", optimizer_file=\"optimizer_params.txt\"):\n","    # Read hyperparameters from file\n","    with open(params_file, \"r\") as f:\n","        params = json.load(f)\n","\n","    # Load optimizer parameters\n","    with open(optimizer_file, \"r\") as f:\n","        optimizer_params = json.load(f)\n","\n","    output_size = params['output_size']\n","    hidden_size = params['hidden_size']\n","    batch_size = params['batch_size']\n","    epochs = params['epochs']\n","    lr = params['learning_rate']\n","    patience = params['patience']\n","\n","    model = SentimentRNN(embedding_matrix, hidden_size, output_size)\n","\n","    # Get the optimizer dynamically based on the configuration in optimizer_file\n","    optimizer = get_optimizer(optimizer_params, model)\n","\n","    criterion = nn.CrossEntropyLoss()\n","    scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=3, gamma=0.1)\n","\n","    # Create DataLoaders for training and validation with custom collate_fn for padding\n","    train_loader = DataLoader(TextDataset(train_data, vocab), batch_size=batch_size, shuffle=True, collate_fn=collate_fn)\n","    valid_loader = DataLoader(TextDataset(valid_data, vocab), batch_size=batch_size, collate_fn=collate_fn)\n","\n","    best_val_accuracy = 0\n","    epochs_no_improve = 0\n","\n","    for epoch in range(epochs):\n","        model.train()\n","        running_loss = 0.0\n","        for inputs, labels in train_loader:\n","            optimizer.zero_grad()\n","            outputs = model(inputs)\n","            loss = criterion(outputs, labels)\n","            loss.backward()\n","            optimizer.step()\n","            running_loss += loss.item()\n","\n","        print(f'Epoch {epoch+1}, Loss: {running_loss/len(train_loader)}')\n","\n","        # Validation\n","        model.eval()\n","        correct = 0\n","        total = 0\n","        with torch.no_grad():\n","            for inputs, labels in valid_loader:\n","                outputs = model(inputs)\n","                _, predicted = torch.max(outputs.data, 1)\n","                total += labels.size(0)\n","                correct += (predicted == labels).sum().item()\n","\n","        val_accuracy = 100 * correct / total\n","        print(f'Validation Accuracy: {val_accuracy}%')\n","\n","        # Early stopping logic: check if validation accuracy improved\n","        if val_accuracy > best_val_accuracy:\n","            best_val_accuracy = val_accuracy\n","            epochs_no_improve = 0  # Reset the patience counter\n","            torch.save(model.state_dict(), 'best_model.pt')  # Save the best model's weights\n","        else:\n","            epochs_no_improve += 1\n","\n","        scheduler.step()  # Step the learning rate scheduler\n","\n","        if epochs_no_improve >= patience:\n","            print(f'Early stopping at epoch {epoch+1}')\n","            break\n","\n","        print(\"Evaluating model on test set...\")\n","        test_accuracy = evaluate_model_on_test(model, test_dataset, vocab)\n","\n","    return model\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"oi8X6VzKnPv4"},"outputs":[],"source":["def evaluate_model_on_test(model, test_data, vocab):\n","    # Create a DataLoader for the test dataset\n","    test_loader = DataLoader(TextDataset(test_data, vocab), batch_size=32, collate_fn=collate_fn)\n","\n","    model.eval()  # Set the model to evaluation mode\n","    correct = 0\n","    total = 0\n","    with torch.no_grad():\n","        for inputs, labels in test_loader:\n","            outputs = model(inputs)\n","            _, predicted = torch.max(outputs.data, 1)\n","            total += labels.size(0)\n","            correct += (predicted == labels).sum().item()\n","\n","    # Calculate and print the test accuracy\n","    test_accuracy = 100 * correct / total\n","    print(f'Test Accuracy: {test_accuracy}%')\n","    return test_accuracy"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"hHPGjUItqvjU"},"outputs":[],"source":[]}],"metadata":{"colab":{"provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.12.5"}},"nbformat":4,"nbformat_minor":0}
