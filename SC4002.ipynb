{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Abeszz/SC4002-NLP-Assignment/blob/main/SC4002.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fRPGfwCv_acw"
      },
      "source": [
        "# Installation & Requirements :"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "J81YDyhL_mX5",
        "outputId": "9fc684bb-e410-484f-b372-bcef54d9ba95"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[33mDEPRECATION: Configuring installation scheme with distutils config files is deprecated and will no longer work in the near future. If you are using a Homebrew or Linuxbrew Python, please see discussion at https://github.com/Homebrew/homebrew-core/issues/76621\u001b[0m\u001b[33m\n",
            "\u001b[0mRequirement already satisfied: datasets in /opt/homebrew/lib/python3.9/site-packages (3.0.2)\n",
            "Requirement already satisfied: filelock in /opt/homebrew/lib/python3.9/site-packages (from datasets) (3.12.4)\n",
            "Requirement already satisfied: numpy>=1.17 in /opt/homebrew/lib/python3.9/site-packages (from datasets) (1.25.2)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /opt/homebrew/lib/python3.9/site-packages (from datasets) (17.0.0)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /opt/homebrew/lib/python3.9/site-packages (from datasets) (0.3.8)\n",
            "Requirement already satisfied: pandas in /opt/homebrew/lib/python3.9/site-packages (from datasets) (2.0.3)\n",
            "Requirement already satisfied: requests>=2.32.2 in /opt/homebrew/lib/python3.9/site-packages (from datasets) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.66.3 in /opt/homebrew/lib/python3.9/site-packages (from datasets) (4.66.5)\n",
            "Requirement already satisfied: xxhash in /opt/homebrew/lib/python3.9/site-packages (from datasets) (3.5.0)\n",
            "Requirement already satisfied: multiprocess<0.70.17 in /opt/homebrew/lib/python3.9/site-packages (from datasets) (0.70.16)\n",
            "Requirement already satisfied: fsspec<=2024.9.0,>=2023.1.0 in /opt/homebrew/lib/python3.9/site-packages (from fsspec[http]<=2024.9.0,>=2023.1.0->datasets) (2023.9.2)\n",
            "Requirement already satisfied: aiohttp in /opt/homebrew/lib/python3.9/site-packages (from datasets) (3.8.5)\n",
            "Requirement already satisfied: huggingface-hub>=0.23.0 in /opt/homebrew/lib/python3.9/site-packages (from datasets) (0.26.1)\n",
            "Requirement already satisfied: packaging in /opt/homebrew/lib/python3.9/site-packages (from datasets) (23.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /opt/homebrew/lib/python3.9/site-packages (from datasets) (6.0.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /opt/homebrew/lib/python3.9/site-packages (from aiohttp->datasets) (24.2.0)\n",
            "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in /opt/homebrew/lib/python3.9/site-packages (from aiohttp->datasets) (3.2.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /opt/homebrew/lib/python3.9/site-packages (from aiohttp->datasets) (6.0.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /opt/homebrew/lib/python3.9/site-packages (from aiohttp->datasets) (4.0.3)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /opt/homebrew/lib/python3.9/site-packages (from aiohttp->datasets) (1.9.2)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /opt/homebrew/lib/python3.9/site-packages (from aiohttp->datasets) (1.4.0)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /opt/homebrew/lib/python3.9/site-packages (from aiohttp->datasets) (1.3.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/homebrew/lib/python3.9/site-packages (from huggingface-hub>=0.23.0->datasets) (4.12.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /opt/homebrew/lib/python3.9/site-packages (from requests>=2.32.2->datasets) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/homebrew/lib/python3.9/site-packages (from requests>=2.32.2->datasets) (2.0.5)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /opt/homebrew/lib/python3.9/site-packages (from requests>=2.32.2->datasets) (2023.7.22)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /opt/homebrew/lib/python3.9/site-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /opt/homebrew/lib/python3.9/site-packages (from pandas->datasets) (2023.3)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /opt/homebrew/lib/python3.9/site-packages (from pandas->datasets) (2023.3)\n",
            "Requirement already satisfied: six>=1.5 in /opt/homebrew/lib/python3.9/site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n",
            "\u001b[33mDEPRECATION: Configuring installation scheme with distutils config files is deprecated and will no longer work in the near future. If you are using a Homebrew or Linuxbrew Python, please see discussion at https://github.com/Homebrew/homebrew-core/issues/76621\u001b[0m\u001b[33m\n",
            "\u001b[0m\n",
            "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.0\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.2\u001b[0m\n",
            "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython3.9 -m pip install --upgrade pip\u001b[0m\n",
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        }
      ],
      "source": [
        "pip install datasets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "ZZ7IipO2Bmo1",
        "outputId": "ad2cb880-8c6f-455d-9418-45f76a69e52e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[33mDEPRECATION: Configuring installation scheme with distutils config files is deprecated and will no longer work in the near future. If you are using a Homebrew or Linuxbrew Python, please see discussion at https://github.com/Homebrew/homebrew-core/issues/76621\u001b[0m\u001b[33m\n",
            "\u001b[0mRequirement already satisfied: nltk in /opt/homebrew/lib/python3.9/site-packages (3.8.1)\n",
            "Requirement already satisfied: click in /opt/homebrew/lib/python3.9/site-packages (from nltk) (8.1.7)\n",
            "Requirement already satisfied: joblib in /opt/homebrew/lib/python3.9/site-packages (from nltk) (1.3.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /opt/homebrew/lib/python3.9/site-packages (from nltk) (2023.8.8)\n",
            "Requirement already satisfied: tqdm in /opt/homebrew/lib/python3.9/site-packages (from nltk) (4.66.5)\n",
            "\u001b[33mDEPRECATION: Configuring installation scheme with distutils config files is deprecated and will no longer work in the near future. If you are using a Homebrew or Linuxbrew Python, please see discussion at https://github.com/Homebrew/homebrew-core/issues/76621\u001b[0m\u001b[33m\n",
            "\u001b[0m\n",
            "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.0\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.2\u001b[0m\n",
            "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython3.9 -m pip install --upgrade pip\u001b[0m\n",
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        }
      ],
      "source": [
        "pip install nltk"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "wzYNgiwEBtoL",
        "outputId": "c92815ec-80c8-4c93-9a1d-c84e2810aa97"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[33mDEPRECATION: Configuring installation scheme with distutils config files is deprecated and will no longer work in the near future. If you are using a Homebrew or Linuxbrew Python, please see discussion at https://github.com/Homebrew/homebrew-core/issues/76621\u001b[0m\u001b[33m\n",
            "\u001b[0mRequirement already satisfied: numpy in /opt/homebrew/lib/python3.9/site-packages (1.25.2)\n",
            "\u001b[33mDEPRECATION: Configuring installation scheme with distutils config files is deprecated and will no longer work in the near future. If you are using a Homebrew or Linuxbrew Python, please see discussion at https://github.com/Homebrew/homebrew-core/issues/76621\u001b[0m\u001b[33m\n",
            "\u001b[0m\n",
            "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.0\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.2\u001b[0m\n",
            "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython3.9 -m pip install --upgrade pip\u001b[0m\n",
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        }
      ],
      "source": [
        "pip install numpy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "3pFMpM5qpLFn",
        "outputId": "638d9c8d-5d98-420b-8a33-945d350d1d4a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[33mDEPRECATION: Configuring installation scheme with distutils config files is deprecated and will no longer work in the near future. If you are using a Homebrew or Linuxbrew Python, please see discussion at https://github.com/Homebrew/homebrew-core/issues/76621\u001b[0m\u001b[33m\n",
            "\u001b[0mRequirement already satisfied: npm in /opt/homebrew/lib/python3.9/site-packages (0.1.1)\n",
            "Requirement already satisfied: optional-django==0.1.0 in /opt/homebrew/lib/python3.9/site-packages (from npm) (0.1.0)\n",
            "\u001b[33mDEPRECATION: Configuring installation scheme with distutils config files is deprecated and will no longer work in the near future. If you are using a Homebrew or Linuxbrew Python, please see discussion at https://github.com/Homebrew/homebrew-core/issues/76621\u001b[0m\u001b[33m\n",
            "\u001b[0m\n",
            "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.0\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.2\u001b[0m\n",
            "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython3.9 -m pip install --upgrade pip\u001b[0m\n",
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        }
      ],
      "source": [
        "pip install npm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "0nqNXGOPywf8",
        "outputId": "c13c2fea-9ddc-4591-8271-ede14fc71776"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[33mDEPRECATION: Configuring installation scheme with distutils config files is deprecated and will no longer work in the near future. If you are using a Homebrew or Linuxbrew Python, please see discussion at https://github.com/Homebrew/homebrew-core/issues/76621\u001b[0m\u001b[33m\n",
            "\u001b[0mRequirement already satisfied: torch in /opt/homebrew/lib/python3.9/site-packages (2.3.0)\n",
            "Requirement already satisfied: filelock in /opt/homebrew/lib/python3.9/site-packages (from torch) (3.12.4)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /opt/homebrew/lib/python3.9/site-packages (from torch) (4.12.2)\n",
            "Requirement already satisfied: sympy in /opt/homebrew/lib/python3.9/site-packages (from torch) (1.12)\n",
            "Requirement already satisfied: networkx in /opt/homebrew/lib/python3.9/site-packages (from torch) (3.1)\n",
            "Requirement already satisfied: jinja2 in /opt/homebrew/lib/python3.9/site-packages (from torch) (3.1.2)\n",
            "Requirement already satisfied: fsspec in /opt/homebrew/lib/python3.9/site-packages (from torch) (2023.9.2)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /opt/homebrew/lib/python3.9/site-packages (from jinja2->torch) (2.1.3)\n",
            "Requirement already satisfied: mpmath>=0.19 in /opt/homebrew/lib/python3.9/site-packages (from sympy->torch) (1.3.0)\n",
            "\u001b[33mDEPRECATION: Configuring installation scheme with distutils config files is deprecated and will no longer work in the near future. If you are using a Homebrew or Linuxbrew Python, please see discussion at https://github.com/Homebrew/homebrew-core/issues/76621\u001b[0m\u001b[33m\n",
            "\u001b[0m\n",
            "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.0\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.2\u001b[0m\n",
            "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython3.9 -m pip install --upgrade pip\u001b[0m\n",
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        }
      ],
      "source": [
        "pip install torch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[33mDEPRECATION: Configuring installation scheme with distutils config files is deprecated and will no longer work in the near future. If you are using a Homebrew or Linuxbrew Python, please see discussion at https://github.com/Homebrew/homebrew-core/issues/76621\u001b[0m\u001b[33m\n",
            "\u001b[0mRequirement already satisfied: gdown in /opt/homebrew/lib/python3.9/site-packages (5.2.0)\n",
            "Requirement already satisfied: beautifulsoup4 in /opt/homebrew/lib/python3.9/site-packages (from gdown) (4.12.3)\n",
            "Requirement already satisfied: filelock in /opt/homebrew/lib/python3.9/site-packages (from gdown) (3.12.4)\n",
            "Requirement already satisfied: requests[socks] in /opt/homebrew/lib/python3.9/site-packages (from gdown) (2.32.3)\n",
            "Requirement already satisfied: tqdm in /opt/homebrew/lib/python3.9/site-packages (from gdown) (4.66.5)\n",
            "Requirement already satisfied: soupsieve>1.2 in /opt/homebrew/lib/python3.9/site-packages (from beautifulsoup4->gdown) (2.6)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/homebrew/lib/python3.9/site-packages (from requests[socks]->gdown) (3.2.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /opt/homebrew/lib/python3.9/site-packages (from requests[socks]->gdown) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/homebrew/lib/python3.9/site-packages (from requests[socks]->gdown) (2.0.5)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /opt/homebrew/lib/python3.9/site-packages (from requests[socks]->gdown) (2023.7.22)\n",
            "Requirement already satisfied: PySocks!=1.5.7,>=1.5.6 in /opt/homebrew/lib/python3.9/site-packages (from requests[socks]->gdown) (1.7.1)\n",
            "\u001b[33mDEPRECATION: Configuring installation scheme with distutils config files is deprecated and will no longer work in the near future. If you are using a Homebrew or Linuxbrew Python, please see discussion at https://github.com/Homebrew/homebrew-core/issues/76621\u001b[0m\u001b[33m\n",
            "\u001b[0m\n",
            "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.0\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.2\u001b[0m\n",
            "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython3.9 -m pip install --upgrade pip\u001b[0m\n",
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        }
      ],
      "source": [
        "pip install gdown"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "IFIJIoS9DuVa",
        "outputId": "4f1b709f-e539-43ef-8641-d723e2cfcfd8"
      },
      "outputs": [],
      "source": [
        "# glove_file_id = '17CUd7jxuh6ptIljKaz_9gJQ8-40JXJ-F'\n",
        "# glove_file = 'glove.6B.100d.txt'\n",
        "# !gdown {glove_file_id} -O {glove_file}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "B8qRXCT2GpKd",
        "outputId": "4aaf10a4-8bcb-404e-f395-c1a84191c940"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /opt/homebrew/opt/python@3.9/\n",
            "[nltk_data]     Frameworks/Python.framework/Versions/3.9/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package punkt_tab to /opt/homebrew/opt/python@\n",
            "[nltk_data]     3.9/Frameworks/Python.framework/Versions/3.9/nltk_data\n",
            "[nltk_data]     ...\n",
            "[nltk_data]   Package punkt_tab is already up-to-date!\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 8,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import os\n",
        "import nltk\n",
        "import sys\n",
        "\n",
        "env_base_path = sys.prefix\n",
        "nltk_path = os.path.join(env_base_path, 'nltk_data')\n",
        "nltk.download('punkt', nltk_path)\n",
        "nltk.download('punkt_tab', nltk_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iBA7sNlf9hW5",
        "outputId": "99cdcdfb-e1a5-48b7-e402-112617a27eb2"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/opt/homebrew/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
            "  from .autonotebook import tqdm as notebook_tqdm\n"
          ]
        }
      ],
      "source": [
        "from datasets import load_dataset\n",
        "dataset = load_dataset('rotten_tomatoes')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "o_TvlwmgB35t"
      },
      "outputs": [],
      "source": [
        "train_dataset = dataset['train']\n",
        "validation_dataset = dataset['validation']\n",
        "test_dataset = dataset['test']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "cBY500zfFszo"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "import numpy as np\n",
        "from nltk.tokenize import word_tokenize\n",
        "from collections import Counter\n",
        "import csv"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "rDD4AHuqWu_h"
      },
      "outputs": [],
      "source": [
        "# Global variables\n",
        "UNKNOWN_TOKEN = '<UNKNOWN>'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "9_o0ZGgaXLbm"
      },
      "outputs": [],
      "source": [
        "# Functions to build vocabulary and create embedding matrix\n",
        "def build_vocabulary(dataset, oov_handling_method='unknown_token'):\n",
        "    vocab_counter = Counter()\n",
        "    for sample in dataset:\n",
        "        tokens = word_tokenize(sample['text'].lower())\n",
        "        vocab_counter.update(tokens)\n",
        "    vocabulary = list(vocab_counter.keys())\n",
        "    if oov_handling_method == 'unknown_token':\n",
        "        if UNKNOWN_TOKEN not in vocabulary:\n",
        "            vocabulary.append(UNKNOWN_TOKEN)\n",
        "    return vocabulary\n",
        "\n",
        "def create_embedding_matrix(embedding_dim, vocabulary, glove_embeddings, oov_handling_method='unknown_token'):\n",
        "    vocab_size = len(vocabulary)\n",
        "    embedding_matrix = np.zeros((vocab_size, embedding_dim))\n",
        "    word_to_index = {word: idx for idx, word in enumerate(vocabulary)}\n",
        "\n",
        "    # Initialize special embeddings\n",
        "    if oov_handling_method == 'unknown_token':\n",
        "        # Use a single <UNKNOWN> token for all OOV words\n",
        "        unknown_vector = np.random.normal(scale=0.6, size=(embedding_dim,))\n",
        "        unknown_index = word_to_index[UNKNOWN_TOKEN]\n",
        "        embedding_matrix[unknown_index] = unknown_vector\n",
        "\n",
        "    # Fill the embedding matrix\n",
        "    for word, idx in word_to_index.items():\n",
        "        if word in glove_embeddings:\n",
        "            embedding_matrix[idx] = glove_embeddings[word]\n",
        "        else:\n",
        "            if oov_handling_method == 'unknown_token':\n",
        "                embedding_matrix[idx] = embedding_matrix[unknown_index]\n",
        "            elif oov_handling_method == 'random':\n",
        "                embedding_matrix[idx] = np.random.normal(scale=0.6, size=(embedding_dim,))\n",
        "            elif oov_handling_method == 'none':\n",
        "                embedding_matrix[idx] = np.zeros(embedding_dim)\n",
        "\n",
        "    return embedding_matrix\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "uzbnx6HrXLQH"
      },
      "outputs": [],
      "source": [
        "# Function to load GloVe embeddings\n",
        "def load_glove_embeddings(glove_file_path):\n",
        "    embeddings_index = {}\n",
        "    with open(glove_file_path, 'r', encoding='utf-8') as f:\n",
        "        for line in f:\n",
        "            values = line.strip().split()\n",
        "            word = values[0]\n",
        "            vector = np.asarray(values[1:], dtype='float32')\n",
        "            embeddings_index[word] = vector\n",
        "    return embeddings_index"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "ymd6E3mqWzvM"
      },
      "outputs": [],
      "source": [
        "# TextDataset class for loading data\n",
        "class TextDataset(Dataset):\n",
        "    def __init__(self, dataset, vocabulary, word_to_index, oov_handling_method='unknown_token'):\n",
        "        self.dataset = dataset\n",
        "        self.vocabulary = vocabulary\n",
        "        self.word_to_index = word_to_index\n",
        "        self.oov_handling_method = oov_handling_method\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.dataset)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        sentence = self.dataset[idx]['text']\n",
        "        label = self.dataset[idx]['label']\n",
        "        tokens = word_tokenize(sentence.lower())\n",
        "\n",
        "        indices = []\n",
        "        for token in tokens:\n",
        "            if token in self.word_to_index:\n",
        "                indices.append(self.word_to_index[token])\n",
        "            else:\n",
        "                if self.oov_handling_method == 'unknown_token':\n",
        "                    indices.append(self.word_to_index[UNKNOWN_TOKEN])\n",
        "                elif self.oov_handling_method == 'random':\n",
        "                    indices.append(0)  # Placeholder index for OOV\n",
        "                    oov_flags.append(1)  # Mark position as OOV\n",
        "                elif self.oov_handling_method == 'none':\n",
        "                    # Skip the word or handle as desired\n",
        "                    continue\n",
        "        if self.oov_handling_method == 'random':\n",
        "            oov_flags = torch.tensor(oov_flags, dtype=torch.bool)\n",
        "        else:\n",
        "            oov_flags = None\n",
        "        return torch.tensor(indices, dtype=torch.long), torch.tensor(label, dtype=torch.long), oov_flags\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "OQSs-xzUW6wp"
      },
      "outputs": [],
      "source": [
        "# Custom collate functions\n",
        "def collate_fn(batch):\n",
        "    inputs, labels, oov_flags = zip(*batch)\n",
        "\n",
        "    inputs = [x for x in inputs]\n",
        "    labels = torch.stack(labels)\n",
        "\n",
        "    padded_inputs = pad_sequence(inputs, batch_first=True, padding_value=0)\n",
        "\n",
        "    # Handle oov_flags\n",
        "    if oov_flags[0] is not None:\n",
        "        oov_flags = [x for x in oov_flags]\n",
        "        padded_oov_flags = pad_sequence(oov_flags, batch_first=True, padding_value=0)\n",
        "    else:\n",
        "        padded_oov_flags = None\n",
        "\n",
        "    return padded_inputs, labels, padded_oov_flags\n",
        "\n",
        "def collate_fn_cnn(batch, max_length=100):\n",
        "    inputs, labels, oov_flags = zip(*batch)\n",
        "    labels = torch.stack(labels)\n",
        "\n",
        "    processed_inputs = []\n",
        "    processed_oov_flags = []\n",
        "    for input_seq, oov_flag_seq in zip(inputs, oov_flags):\n",
        "        seq_len = len(input_seq)\n",
        "        if seq_len >= max_length:\n",
        "            processed_inputs.append(input_seq[:max_length])\n",
        "            if oov_flag_seq is not None:\n",
        "                processed_oov_flags.append(oov_flag_seq[:max_length])\n",
        "            else:\n",
        "                processed_oov_flags.append(torch.zeros(max_length, dtype=torch.bool))\n",
        "        else:\n",
        "            pad_len = max_length - seq_len\n",
        "            processed_inputs.append(torch.cat([input_seq, torch.zeros(pad_len, dtype=torch.long)]))\n",
        "            if oov_flag_seq is not None:\n",
        "                processed_oov_flags.append(torch.cat([oov_flag_seq, torch.zeros(pad_len, dtype=torch.bool)]))\n",
        "            else:\n",
        "                processed_oov_flags.append(torch.zeros(max_length, dtype=torch.bool))\n",
        "\n",
        "    # Ensure inputs are LongTensor for embedding\n",
        "    inputs = torch.stack(processed_inputs).long()\n",
        "    if oov_flags[0] is not None:\n",
        "        oov_flags = torch.stack(processed_oov_flags)\n",
        "    else:\n",
        "        oov_flags = None\n",
        "\n",
        "    return inputs, labels, oov_flags\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "uEY6zYYKW6t1"
      },
      "outputs": [],
      "source": [
        "# SentimentRNN class\n",
        "class SentimentRNN(nn.Module):\n",
        "    def __init__(self, embedding_matrix, hidden_size, output_size,\n",
        "                 rnn_type=\"RNN\", num_layers=1, use_bidirectional=False,\n",
        "                 use_dropout=False, use_batch_norm=False, use_layer_norm=False,\n",
        "                 aggregation_method='last_hidden', freeze_embeddings=True):\n",
        "        super(SentimentRNN, self).__init__()\n",
        "\n",
        "        vocab_size, embedding_dim = embedding_matrix.shape\n",
        "\n",
        "        # Embedding layer using pre-trained embeddings\n",
        "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
        "        self.embedding.weight = nn.Parameter(torch.tensor(embedding_matrix, dtype=torch.float32))\n",
        "        self.embedding.weight.requires_grad = not freeze_embeddings  # Control freezing\n",
        "\n",
        "        # Choose RNN type dynamically\n",
        "        if rnn_type == \"LSTM\":\n",
        "            self.rnn = nn.LSTM(embedding_dim, hidden_size, num_layers=num_layers,\n",
        "                               batch_first=True, bidirectional=use_bidirectional)\n",
        "        elif rnn_type == \"GRU\":\n",
        "            self.rnn = nn.GRU(embedding_dim, hidden_size, num_layers=num_layers,\n",
        "                              batch_first=True, bidirectional=use_bidirectional)\n",
        "        else:  # Default to Simple RNN\n",
        "            self.rnn = nn.RNN(embedding_dim, hidden_size, num_layers=num_layers,\n",
        "                              batch_first=True, bidirectional=use_bidirectional)\n",
        "\n",
        "        # Store the aggregation method\n",
        "        self.aggregation_method = aggregation_method\n",
        "\n",
        "        # Determine the final hidden size after aggregation\n",
        "        if aggregation_method == 'last_hidden':\n",
        "            if use_bidirectional:\n",
        "                final_hidden_size = hidden_size * 2\n",
        "            else:\n",
        "                final_hidden_size = hidden_size\n",
        "        else:\n",
        "            if use_bidirectional:\n",
        "                final_hidden_size = hidden_size * 2\n",
        "            else:\n",
        "                final_hidden_size = hidden_size\n",
        "\n",
        "        # Fully connected layer for classification\n",
        "        self.fc = nn.Linear(final_hidden_size, output_size)\n",
        "\n",
        "        # Optional Regularization Layers\n",
        "        self.use_dropout = use_dropout\n",
        "        self.use_batch_norm = use_batch_norm\n",
        "        self.use_layer_norm = use_layer_norm\n",
        "\n",
        "        if self.use_dropout:\n",
        "            self.dropout = nn.Dropout(0.3)  # Dropout rate of 0.3\n",
        "\n",
        "        if self.use_batch_norm:\n",
        "            self.batch_norm = nn.BatchNorm1d(final_hidden_size)\n",
        "\n",
        "        if self.use_layer_norm:\n",
        "            self.layer_norm = nn.LayerNorm(final_hidden_size)\n",
        "\n",
        "    def forward(self, x, oov_flags=None):\n",
        "        embedded = self.embedding(x)\n",
        "        batch_size, seq_length, embedding_dim = embedded.shape\n",
        "\n",
        "        if oov_flags is not None:\n",
        "            # Replace embeddings at OOV positions with random embeddings\n",
        "            random_embeddings = torch.randn(batch_size, seq_length, embedding_dim, device=embedded.device) * 0.6\n",
        "            oov_flags = oov_flags.unsqueeze(-1).expand_as(embedded)\n",
        "            embedded = torch.where(oov_flags, random_embeddings, embedded)\n",
        "\n",
        "        output, hidden = self.rnn(embedded)\n",
        "\n",
        "        # For LSTM, hidden is a tuple of (h_n, c_n); use h_n\n",
        "        if isinstance(hidden, tuple):\n",
        "            hidden = hidden[0]  # h_n\n",
        "\n",
        "        if self.aggregation_method == 'last_hidden':\n",
        "            if self.rnn.bidirectional:\n",
        "                final_output = torch.cat((hidden[-2,:,:], hidden[-1,:,:]), dim=1)\n",
        "            else:\n",
        "                final_output = hidden[-1,:,:]\n",
        "        elif self.aggregation_method == 'mean_pooling':\n",
        "            final_output = output.mean(dim=1)\n",
        "        elif self.aggregation_method == 'max_pooling':\n",
        "            final_output, _ = torch.max(output, dim=1)\n",
        "        else:\n",
        "            raise ValueError(f\"Unknown aggregation method: {self.aggregation_method}\")\n",
        "\n",
        "        # Apply optional regularization layers\n",
        "        if self.use_batch_norm:\n",
        "            final_output = self.batch_norm(final_output)\n",
        "\n",
        "        if self.use_layer_norm:\n",
        "            final_output = self.layer_norm(final_output)\n",
        "\n",
        "        if self.use_dropout:\n",
        "            final_output = self.dropout(final_output)\n",
        "\n",
        "        return self.fc(final_output)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "enikXnhwW6rA"
      },
      "outputs": [],
      "source": [
        "# SentimentCNN class\n",
        "class SentimentCNN(nn.Module):\n",
        "    def __init__(self, embedding_matrix, output_size, freeze_embeddings=True,\n",
        "                 num_filters=100, filter_sizes=[3,4,5], dropout_rate=0.5):\n",
        "        super(SentimentCNN, self).__init__()\n",
        "\n",
        "        vocab_size, embedding_dim = embedding_matrix.shape\n",
        "\n",
        "        # Embedding layer using pre-trained embeddings\n",
        "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
        "        self.embedding.weight = nn.Parameter(torch.tensor(embedding_matrix, dtype=torch.float32))\n",
        "        self.embedding.weight.requires_grad = not freeze_embeddings  # Control freezing\n",
        "\n",
        "        # Convolutional layers with multiple filter sizes\n",
        "        self.convs = nn.ModuleList([\n",
        "            nn.Conv2d(in_channels=1, out_channels=num_filters, kernel_size=(fs, embedding_dim))\n",
        "            for fs in filter_sizes\n",
        "        ])\n",
        "\n",
        "        # Fully connected layer\n",
        "        self.fc = nn.Linear(len(filter_sizes) * num_filters, output_size)\n",
        "\n",
        "        # Dropout layer\n",
        "        self.dropout = nn.Dropout(dropout_rate)\n",
        "\n",
        "    def forward(self, x, oov_flags=None):\n",
        "        embedded = self.embedding(x)\n",
        "        batch_size, seq_length, embedding_dim = embedded.shape\n",
        "\n",
        "        if oov_flags is not None:\n",
        "            # Replace embeddings at OOV positions with random embeddings\n",
        "            random_embeddings = torch.randn(batch_size, seq_length, embedding_dim, device=embedded.device) * 0.6\n",
        "            oov_flags = oov_flags.unsqueeze(-1).expand_as(embedded)\n",
        "            embedded = torch.where(oov_flags, random_embeddings, embedded)\n",
        "\n",
        "        embedded = embedded.unsqueeze(1)  # Add channel dimension\n",
        "\n",
        "        # Apply convolution and ReLU activation\n",
        "        conv_outs = [torch.relu(conv(embedded)).squeeze(3) for conv in self.convs]\n",
        "\n",
        "        # Apply max pooling over the sequence length\n",
        "        pooled_outs = [torch.max(conv_out, dim=2)[0] for conv_out in conv_outs]\n",
        "\n",
        "        # Concatenate pooled outputs\n",
        "        cat = torch.cat(pooled_outs, dim=1)\n",
        "\n",
        "        # Apply dropout\n",
        "        out = self.dropout(cat)\n",
        "\n",
        "        # Fully connected layer\n",
        "        out = self.fc(out)\n",
        "        return out\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "zjircnwJW6oR"
      },
      "outputs": [],
      "source": [
        "# Function to get optimizer\n",
        "def get_optimizer(params, model):\n",
        "    optimizer_type = params[\"optimizer_type\"]\n",
        "    lr = params[\"learning_rate\"]\n",
        "    weight_decay = params.get(\"weight_decay\", 0)  # Default to 0 if not specified\n",
        "\n",
        "    if optimizer_type == \"SGD\":\n",
        "        momentum = params.get(\"momentum\", 0)  # Default to 0 if not specified\n",
        "        return optim.SGD(model.parameters(), lr=lr, momentum=momentum, weight_decay=weight_decay)\n",
        "\n",
        "    elif optimizer_type == \"Adam\":\n",
        "        return optim.Adam(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
        "\n",
        "    else:\n",
        "        raise ValueError(f\"Unknown optimizer type: {optimizer_type}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "7jvCWISIW6im"
      },
      "outputs": [],
      "source": [
        "# Function to train the model\n",
        "def train_model(model, train_loader, valid_loader, test_loader, optimizer, epochs, patience, scheduler_step_size, scheduler_gamma, device):\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=scheduler_step_size, gamma=scheduler_gamma)\n",
        "    \n",
        "    train_losses = []\n",
        "    val_accuracies = []\n",
        "    test_accuracies = []\n",
        "    best_val_accuracy = 0\n",
        "    epochs_no_improve = 0\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        model.train()\n",
        "        running_loss = 0.0\n",
        "        for inputs, labels, oov_flags in train_loader:\n",
        "            inputs, labels = inputs.to(device), labels.to(device)\n",
        "            if oov_flags is not None:\n",
        "                oov_flags = oov_flags.to(device)\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(inputs, oov_flags)\n",
        "            loss = criterion(outputs, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            running_loss += loss.item()\n",
        "\n",
        "        avg_train_loss = running_loss / len(train_loader)\n",
        "        train_losses.append(avg_train_loss)\n",
        "\n",
        "        # Validation\n",
        "        val_accuracy = evaluate_accuracy(model, valid_loader, device)\n",
        "        val_accuracies.append(val_accuracy)\n",
        "\n",
        "        print(f'Epoch {epoch}, Loss: {avg_train_loss:.4f}, Validation Accuracy: {val_accuracy:.2f}%')\n",
        "\n",
        "        # Early stopping logic\n",
        "        if val_accuracy > best_val_accuracy:\n",
        "            best_val_accuracy = val_accuracy\n",
        "            epochs_no_improve = 0\n",
        "            # Save the best model\n",
        "            torch.save(model.state_dict(), f'best_model.pt')\n",
        "            print(\"best_epoch: \", epoch)\n",
        "        else:\n",
        "            epochs_no_improve += 1\n",
        "\n",
        "        if epochs_no_improve >= patience:\n",
        "            print(f\"Early stopping at epoch {epoch}\")\n",
        "            break\n",
        "\n",
        "        scheduler.step()\n",
        "\n",
        "    # Load the best model before evaluating on test set\n",
        "    # best_epoch = val_accuracies.index(best_val_accuracy) + 1\n",
        "    model.load_state_dict(torch.load(f'best_model.pt'))\n",
        "\n",
        "    # Test Accuracy\n",
        "    test_accuracy = evaluate_accuracy(model, test_loader, device)\n",
        "    test_accuracies = [test_accuracy] * len(train_losses)\n",
        "    print(f'Test Accuracy: {test_accuracy:.2f}%')\n",
        "\n",
        "    return train_losses, val_accuracies, test_accuracies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "4dvJWIl8W6Z5"
      },
      "outputs": [],
      "source": [
        "# Function to evaluate accuracy\n",
        "def evaluate_accuracy(model, data_loader, device):\n",
        "    model.eval()\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    with torch.no_grad():\n",
        "        for inputs, labels, oov_flags in data_loader:\n",
        "            inputs, labels = inputs.to(device), labels.to(device)\n",
        "            if oov_flags is not None:\n",
        "                oov_flags = oov_flags.to(device)\n",
        "            outputs = model(inputs, oov_flags)\n",
        "            _, predicted = outputs.max(1)\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "    accuracy = 100 * correct / total\n",
        "    return accuracy\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.model_selection import ParameterGrid\n",
        "\n",
        "# test function param\n",
        "# part_2_rnn_param_grid = { # rnn, fixed embeddings no OOV soln\n",
        "#     \"optimizer_type\": [\"Adam\"],\n",
        "#     \"learning_rate\": [0.01],\n",
        "#     \"momentum\": [0],  # Used only for SGD\n",
        "#     \"weight_decay\": [0.0001],\n",
        "#     \"batch_size\": [64],\n",
        "#     \"epochs\": [10],\n",
        "#     \"patience\": [10],\n",
        "\n",
        "#     \"model_type\": [\"RNN\"],\n",
        "#     \"rnn_type\": [\"RNN\"],\n",
        "\n",
        "#     \"num_layers\": [1],\n",
        "#     \"use_bidirectional\": [False],\n",
        "#     \"use_dropout\": [True],\n",
        "#     \"use_batch_norm\": [True],\n",
        "#     \"use_layer_norm\": [True],\n",
        "#     \"aggregation_method\": [\"last_hidden\"],\n",
        "    \n",
        "#     \"hidden_size\": [64],\n",
        "#     \"output_size\": [2],\n",
        "#     \"freeze_embeddings\": [True], # keep embeddings fixed\n",
        "\n",
        "#     \"oov_handling_method\": [\"none\"], # no OOV soln\n",
        "#     \"embedding_dim\": [100], # default\n",
        "#     \"glove_file_path\": [\"glove.6B.100d.txt\"] # default\n",
        "# }\n",
        "part_2_rnn_param_grid = { # rnn, fixed embeddings no OOV soln\n",
        "    \"optimizer_type\": [\"Adam\", \"SGD\"],\n",
        "    \"learning_rate\": [0.01, 0.001],\n",
        "    \"momentum\": [0, 0.9],  # Used only for SGD\n",
        "    \"weight_decay\": [0.0001, 0.001],\n",
        "    \"batch_size\": [32, 64, 128],\n",
        "    \"epochs\": [100, 200],\n",
        "    \"patience\": [10],\n",
        "\n",
        "    \"model_type\": [\"RNN\"],\n",
        "    \"rnn_type\": [\"RNN\"],\n",
        "\n",
        "    \"num_layers\": [1, 2, 3],\n",
        "    \"use_bidirectional\": [False],\n",
        "    \"use_dropout\": [True, False],\n",
        "    \"use_batch_norm\": [True, False],\n",
        "    \"use_layer_norm\": [True, False],\n",
        "    \"aggregation_method\": [\"last_hidden\"],\n",
        "    \n",
        "    \"hidden_size\": [64, 128, 256],\n",
        "    \"output_size\": [2],\n",
        "    \"freeze_embeddings\": [True], # keep embeddings fixed\n",
        "\n",
        "    \"oov_handling_method\": [\"none\"], # no OOV soln\n",
        "    \"embedding_dim\": [100], # default\n",
        "    \"glove_file_path\": [\"glove.6B.100d.txt\"] # default\n",
        "}\n",
        "\n",
        "part_3_rnn_param_grid = { # rnn, update embeddings with OOV soln\n",
        "    \"optimizer_type\": [\"Adam\", \"SGD\"],\n",
        "    \"learning_rate\": [0.01, 0.001],\n",
        "    \"momentum\": [0.9, 0.95],  # Used only for SGD\n",
        "    \"weight_decay\": [0.0001, 0.001],\n",
        "    \"batch_size\": [32, 64, 128],\n",
        "    \"epochs\": [100, 200],\n",
        "    \"patience\": [10],\n",
        "\n",
        "    \"model_type\": [\"RNN\"],\n",
        "    \"rnn_type\": [\"RNN\"],\n",
        "\n",
        "    \"num_layers\": [1, 2, 3],\n",
        "    \"use_bidirectional\": [False],\n",
        "    \"use_dropout\": [True],\n",
        "    \"use_batch_norm\": [True, False],\n",
        "    \"use_layer_norm\": [True, False],\n",
        "    \"aggregation_method\": [\"last_hidden\"],\n",
        "    \n",
        "    \"hidden_size\": [64, 128, 256],\n",
        "    \"output_size\": [2],\n",
        "    \"freeze_embeddings\": [False], # update embeddings during training\n",
        "\n",
        "    \"oov_handling_method\": [\"unknown_token\", \"random\"], # apply soln of OOV and train\n",
        "    \"embedding_dim\": [100], # default\n",
        "    \"glove_file_path\": [\"glove.6B.100d.txt\"] # default\n",
        "}\n",
        "\n",
        "part_3_blstm_bgru_param_grid = { # replace rnn with blstm/bgru, update embeddings with OOV soln\n",
        "    \"optimizer_type\": [\"Adam\", \"SGD\"],\n",
        "    \"learning_rate\": [0.01, 0.001],\n",
        "    \"momentum\": [0.9, 0.95],  # Used only for SGD\n",
        "    \"weight_decay\": [0.0001, 0.001],\n",
        "    \"batch_size\": [32, 64, 128],\n",
        "    \"epochs\": [100,200],\n",
        "    \"patience\": [10],\n",
        "\n",
        "    \"model_type\": [\"RNN\"],\n",
        "    \"rnn_type\": [\"LSTM\", \"GRU\"],\n",
        "\n",
        "    \"num_layers\": [1, 2, 3],\n",
        "    \"use_bidirectional\": [True],\n",
        "    \"use_dropout\": [True],\n",
        "    \"use_batch_norm\": [True, False],\n",
        "    \"use_layer_norm\": [True, False],\n",
        "    \"aggregation_method\": [\"last_hidden\", \"mean_pooling\", \"max_pooling\"],\n",
        "    \n",
        "    \"hidden_size\": [64, 128, 256],\n",
        "    \"output_size\": [2],\n",
        "    \"freeze_embeddings\": [False], # update embeddings during training\n",
        "\n",
        "    \"oov_handling_method\": [\"unknown_token\", \"random\"], # apply soln of OOV and train\n",
        "    \"embedding_dim\": [100], # default\n",
        "    \"glove_file_path\": [\"glove.6B.100d.txt\"] # default\n",
        "}\n",
        "\n",
        "part_3_cnn_param_grid = { # replace rnn with cnn, update embeddings with OOV soln\n",
        "    \"optimizer_type\": [\"Adam\", \"SGD\"],\n",
        "    \"learning_rate\": [0.01, 0.001],\n",
        "    \"momentum\": [0.9, 0.95],  # Used only for SGD\n",
        "    \"weight_decay\": [0.0001, 0.001],\n",
        "    \"batch_size\": [32, 64, 128],\n",
        "    \"epochs\": [100, 200],\n",
        "    \"patience\": [10],\n",
        "\n",
        "    \"model_type\": [\"CNN\"],\n",
        "    \n",
        "    \"num_filters\": [100, 128, 256],  # CNN only\n",
        "    \"filter_sizes\": [[3, 4, 5], [2, 3]],\n",
        "    \"dropout_rate\": [0.3, 0.5, 0.7],  # CNN only\n",
        "    \"output_size\": [2],\n",
        "    \"freeze_embeddings\": [False], # update embeddings during training\n",
        "\n",
        "    \"oov_handling_method\": [\"unknown_token\", \"random\"], # apply soln of OOV and train\n",
        "    \"embedding_dim\": [100], # default\n",
        "    \"glove_file_path\": [\"glove.6B.100d.txt\"] # default\n",
        "}\n",
        "\n",
        "# Quest"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {},
      "outputs": [],
      "source": [
        "import itertools\n",
        "import csv"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 56,
      "metadata": {},
      "outputs": [],
      "source": [
        "def run_part2_rnn_experiments_from_paramgrid(train_dataset, validation_dataset, test_dataset):\n",
        "    param_combinations = list(itertools.product(*part_2_rnn_param_grid.values())) # Initialize a list to hold results\n",
        "    results = [] # Initialize a list to hold results\n",
        "\n",
        "    # Iterate through each combination of parameters\n",
        "    for param in param_combinations:\n",
        "        config = dict(zip(part_2_rnn_param_grid.keys(), param))\n",
        "        print(config)\n",
        "\n",
        "        # Load GloVe embeddings\n",
        "        glove_embeddings = load_glove_embeddings(config['glove_file_path'])\n",
        "\n",
        "        # Build vocabulary and create embedding matrix\n",
        "        vocabulary = build_vocabulary(train_dataset, config['oov_handling_method'])\n",
        "        embedding_matrix = create_embedding_matrix(config['embedding_dim'], vocabulary, glove_embeddings, config['oov_handling_method'])\n",
        "\n",
        "        # Build word_to_index mapping\n",
        "        word_to_index = {word: idx for idx, word in enumerate(vocabulary)}\n",
        "        \n",
        "        model, collate_function = check_model_type(model_type=config['model_type'], embedding_matrix=embedding_matrix, config=config)\n",
        "        \n",
        "        \n",
        "        \n",
        "        results = call_training_and_record(config=config, model=model, collate_function=collate_function, vocabulary=vocabulary, word_to_index=word_to_index)\n",
        "\n",
        "    header = list(part_2_rnn_param_grid.keys()) + ['Epoch', 'Train Loss', 'Validation Accuracy', 'Test Accuracy'] # Define CSV header\n",
        "    \n",
        "    save_param_and_perf_into_csv(file_name='part2_rnn_results.csv', header=header, results=results)\n",
        "    "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 57,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Function to run experiments from CSV\n",
        "def run_part3_rnn_experiments_from_paramgrid(train_dataset, validation_dataset, test_dataset):\n",
        "    param_combinations = list(itertools.product(*part_3_rnn_param_grid.values())) # Initialize a list to hold results\n",
        "    results = [] # Initialize a list to hold results\n",
        "    \n",
        "    # Iterate through each combination of parameters\n",
        "    for param in param_combinations:\n",
        "        config = dict(zip(part_3_rnn_param_grid.keys(), param))\n",
        "\n",
        "        # Load GloVe embeddings\n",
        "        glove_embeddings = load_glove_embeddings(config['glove_file_path'])\n",
        "\n",
        "        # Build vocabulary and create embedding matrix\n",
        "        vocabulary = build_vocabulary(train_dataset, config['oov_handling_method'])\n",
        "        embedding_matrix = create_embedding_matrix(config['embedding_dim'], vocabulary, glove_embeddings, config['oov_handling_method'])\n",
        "\n",
        "        # Build word_to_index mapping\n",
        "        word_to_index = {word: idx for idx, word in enumerate(vocabulary)}\n",
        "\n",
        "        model, collate_function = check_model_type(model_type=config['model_type'], embedding_matrix=embedding_matrix, config=config)\n",
        "\n",
        "        results = call_training_and_record(config=config, model=model, collate_function=collate_function, vocabulary=vocabulary, word_to_index=word_to_index)\n",
        "    \n",
        "    header = list(part_3_rnn_param_grid.keys()) + ['Epoch', 'Train Loss', 'Validation Accuracy', 'Test Accuracy'] # Define CSV header\n",
        "    \n",
        "    save_param_and_perf_into_csv(file_name='part3_rnn_results.csv', header=header, results=results)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 58,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Function to run experiments from CSV\n",
        "def run_part3_blstm_bgru_experiments_from_paramgrid(train_dataset, validation_dataset, test_dataset):\n",
        "    param_combinations = list(itertools.product(*part_3_blstm_bgru_param_grid.values())) # Initialize a list to hold results\n",
        "    results = [] # Initialize a list to hold results\n",
        "    \n",
        "    # Iterate through each combination of parameters\n",
        "    for param in param_combinations:\n",
        "        config = dict(zip(part_3_blstm_bgru_param_grid.keys(), param))\n",
        "\n",
        "        # Load GloVe embeddings\n",
        "        glove_embeddings = load_glove_embeddings(config['glove_file_path'])\n",
        "\n",
        "        # Build vocabulary and create embedding matrix\n",
        "        vocabulary = build_vocabulary(train_dataset, config['oov_handling_method'])\n",
        "        embedding_matrix = create_embedding_matrix(config['embedding_dim'], vocabulary, glove_embeddings, config['oov_handling_method'])\n",
        "\n",
        "        # Build word_to_index mapping\n",
        "        word_to_index = {word: idx for idx, word in enumerate(vocabulary)}\n",
        "\n",
        "        model, collate_function = check_model_type(model_type=config['model_type'], embedding_matrix=embedding_matrix, config=config)\n",
        "\n",
        "        results = call_training_and_record(config=config, model=model, collate_function=collate_function, vocabulary=vocabulary, word_to_index=word_to_index)\n",
        "    \n",
        "    header = list(part_3_blstm_bgru_param_grid.keys()) + ['Epoch', 'Train Loss', 'Validation Accuracy', 'Test Accuracy'] # Define CSV header\n",
        "\n",
        "    save_param_and_perf_into_csv(file_name='part3_bsltm_bgru_results.csv', header=header, results=results)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 59,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Function to run experiments from CSV\n",
        "def run_cnn_experiments_from_paramgrid(train_dataset, validation_dataset, test_dataset):\n",
        "    param_combinations = list(itertools.product(*part_3_cnn_param_grid.values())) # Initialize a list to hold results\n",
        "    \n",
        "    \n",
        "    # Iterate through each combination of parameters\n",
        "    for param in param_combinations:\n",
        "        config = dict(zip(part_3_cnn_param_grid.keys(), param))\n",
        "\n",
        "        # Load GloVe embeddings\n",
        "        glove_embeddings = load_glove_embeddings(config['glove_file_path'])\n",
        "\n",
        "        # Build vocabulary and create embedding matrix\n",
        "        vocabulary = build_vocabulary(train_dataset, config['oov_handling_method'])\n",
        "        embedding_matrix = create_embedding_matrix(config['embedding_dim'], vocabulary, glove_embeddings, config['oov_handling_method'])\n",
        "\n",
        "        # Build word_to_index mapping\n",
        "        word_to_index = {word: idx for idx, word in enumerate(vocabulary)}\n",
        "\n",
        "        model, collate_function = check_model_type(model_type=config['model_type'], embedding_matrix=embedding_matrix, config=config)\n",
        "\n",
        "        results = call_training_and_record(config=config, model=model, collate_function=collate_function, vocabulary=vocabulary, word_to_index=word_to_index)\n",
        "        \n",
        "    \n",
        "    header = list(part_3_cnn_param_grid.keys()) + ['Epoch', 'Train Loss', 'Validation Accuracy', 'Test Accuracy'] # Define CSV header\n",
        "    \n",
        "    save_param_and_perf_into_csv(file_name='part3_cnn_results.csv', header=header, results=results)\n",
        "    "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 60,
      "metadata": {},
      "outputs": [],
      "source": [
        "def check_model_type(model_type, embedding_matrix, config):\n",
        "    # Initialize the model based on 'model_type'\n",
        "    if model_type == 'RNN':\n",
        "        model = SentimentRNN(\n",
        "            embedding_matrix=embedding_matrix,\n",
        "            hidden_size=config['hidden_size'],\n",
        "            output_size=config['output_size'],\n",
        "            rnn_type=config['rnn_type'],\n",
        "            num_layers=config['num_layers'],\n",
        "            use_bidirectional=config['use_bidirectional'],\n",
        "            use_dropout=config['use_dropout'],\n",
        "            use_batch_norm=config['use_batch_norm'],\n",
        "            use_layer_norm=config['use_layer_norm'],\n",
        "            aggregation_method=config['aggregation_method'],\n",
        "            freeze_embeddings=config['freeze_embeddings']\n",
        "        )\n",
        "        collate_function = collate_fn\n",
        "        return model, collate_function\n",
        "    elif model_type == 'CNN':\n",
        "        model = SentimentCNN(\n",
        "            embedding_matrix=embedding_matrix,\n",
        "            output_size=config['output_size'],\n",
        "            freeze_embeddings=config['freeze_embeddings'],\n",
        "            num_filters=config['num_filters'],\n",
        "            filter_sizes=config['filter_sizes'],\n",
        "            dropout_rate=config['dropout_rate']\n",
        "        )\n",
        "        collate_function = collate_fn_cnn\n",
        "        return model, collate_function\n",
        "    else:\n",
        "        raise ValueError(f\"Unknown model type: {config['model_type']}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 61,
      "metadata": {},
      "outputs": [],
      "source": [
        "def call_training_and_record(config, model, collate_function, vocabulary, word_to_index):\n",
        "    results = [] # Initialize a list to hold results\n",
        "    \n",
        "    # Get the optimizer dynamically based on the config\n",
        "    optimizer_params = {\n",
        "        'optimizer_type': config['optimizer_type'],\n",
        "        'learning_rate': config['learning_rate'],\n",
        "        'momentum': config['momentum'],\n",
        "        'weight_decay': config['weight_decay']\n",
        "    }\n",
        "    optimizer = get_optimizer(optimizer_params, model)\n",
        "\n",
        "    # Create DataLoaders using the appropriate collate function\n",
        "    train_loader = DataLoader(TextDataset(train_dataset, vocabulary, word_to_index, config['oov_handling_method']), batch_size=config['batch_size'], shuffle=True, collate_fn=collate_function)\n",
        "    valid_loader = DataLoader(TextDataset(validation_dataset, vocabulary, word_to_index, config['oov_handling_method']), batch_size=config['batch_size'], collate_fn=collate_function)\n",
        "    test_loader = DataLoader(TextDataset(test_dataset, vocabulary, word_to_index, config['oov_handling_method']), batch_size=config['batch_size'], collate_fn=collate_function)\n",
        "\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "    model.to(device)\n",
        "    \n",
        "    # Train the model\n",
        "    train_loss, val_accuracy, test_accuracy = train_model(\n",
        "        model=model,\n",
        "        train_loader=train_loader,\n",
        "        valid_loader=valid_loader,\n",
        "        test_loader=test_loader,\n",
        "        optimizer=optimizer,\n",
        "        epochs=config['epochs'],\n",
        "        patience=config['patience'],\n",
        "        scheduler_step_size=3,\n",
        "        scheduler_gamma=0.1,\n",
        "        device=device\n",
        "    )\n",
        "\n",
        "    for i in range(len(train_loss)):  # Iterate through each epoch's results\n",
        "        epoch_result = {\n",
        "            **config,  # Copy the current config\n",
        "            'Epoch': i+1,\n",
        "            'Train Loss': round(train_loss[i], 2),\n",
        "            'Validation Accuracy': round(val_accuracy[i], 2),\n",
        "            'Test Accuracy': round(test_accuracy[i], 2)\n",
        "        }\n",
        "\n",
        "        results.append(epoch_result)  # Append the config to results list\n",
        "    \n",
        "    return results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 62,
      "metadata": {},
      "outputs": [],
      "source": [
        "def save_param_and_perf_into_csv(file_name, header, results):\n",
        "    # Write results to a CSV file\n",
        "    with open(file_name, mode='w', newline='') as csvfile:\n",
        "        writer = csv.DictWriter(csvfile, fieldnames=header)\n",
        "        writer.writeheader()\n",
        "        writer.writerows(results)\n",
        "\n",
        "    print(f\"Parameter combinations and performances recorded in {file_name}.csv\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 63,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5xFaeosRZRSQ",
        "outputId": "260e9e8c-783d-4775-9e90-217e00161c3e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'optimizer_type': 'Adam', 'learning_rate': 0.01, 'momentum': 0, 'weight_decay': 0.0001, 'batch_size': 64, 'epochs': 10, 'patience': 10, 'model_type': 'RNN', 'rnn_type': 'RNN', 'num_layers': 1, 'use_bidirectional': False, 'use_dropout': True, 'use_batch_norm': True, 'use_layer_norm': True, 'aggregation_method': 'last_hidden', 'hidden_size': 64, 'output_size': 2, 'freeze_embeddings': True, 'oov_handling_method': 'none', 'embedding_dim': 100, 'glove_file_path': 'glove.6B.100d.txt'}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/var/folders/g7/dzzn64hs1xldzfp9ks8cdpq40000gn/T/ipykernel_13920/1654731144.py:4: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  inputs = [torch.tensor(x) for x in inputs]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1, Loss: 0.7148, Validation Accuracy: 50.00%\n",
            "best_epoch:  1\n",
            "Epoch 2, Loss: 0.6973, Validation Accuracy: 50.00%\n",
            "Epoch 3, Loss: 0.6960, Validation Accuracy: 49.91%\n",
            "Epoch 4, Loss: 0.6936, Validation Accuracy: 50.00%\n",
            "Epoch 5, Loss: 0.6932, Validation Accuracy: 49.91%\n",
            "Epoch 6, Loss: 0.6936, Validation Accuracy: 50.00%\n",
            "Epoch 7, Loss: 0.6929, Validation Accuracy: 50.00%\n",
            "Epoch 8, Loss: 0.6933, Validation Accuracy: 50.00%\n",
            "Epoch 9, Loss: 0.6932, Validation Accuracy: 50.09%\n",
            "best_epoch:  9\n",
            "Epoch 10, Loss: 0.6931, Validation Accuracy: 50.09%\n",
            "Test Accuracy: 49.72%\n",
            "Parameter combinations and performances recorded in part2_rnn_results.csv.csv\n"
          ]
        }
      ],
      "source": [
        "# run_experiments_from_csv(train_dataset, validation_dataset, test_dataset)\n",
        "run_part2_rnn_experiments_from_paramgrid(train_dataset, validation_dataset, test_dataset)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "r1BNKjKJmco_"
      },
      "outputs": [],
      "source": [
        "#run_part3_rnn_experiments_from_paramgrid(train_dataset, validation_dataset, test_dataset)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#run_part3_blstm_bgru_experiments_from_paramgrid(train_dataset, validation_dataset, test_dataset)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#run_cnn_experiments_from_paramgrid(train_dataset, validation_dataset, test_dataset)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
